{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related works Architecture:\n",
    "\n",
    "- Original Transformer\n",
    "\n",
    "- BERT\n",
    "\n",
    "- BART\n",
    "\n",
    "- T5\n",
    "\n",
    "- DeBERTa\n",
    "\n",
    "- GPT-NeoX\n",
    "\n",
    "- PaLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:37:55.680673Z",
     "iopub.status.busy": "2025-10-29T14:37:55.679902Z",
     "iopub.status.idle": "2025-10-29T14:37:55.684507Z",
     "shell.execute_reply": "2025-10-29T14:37:55.683913Z",
     "shell.execute_reply.started": "2025-10-29T14:37:55.680641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import sys, os\n",
    "import difflib\n",
    "\n",
    "# project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n",
    "# if project_root not in sys.path:\n",
    "#     sys.path.append(project_root)\n",
    "\n",
    "# from shared_functions.global_functions import *\n",
    "# from shared_functions.gg_sheet import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to keep in mind the wseg will not be used here to match with the PhoBERT tokenization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:32:36.215974Z",
     "iopub.status.busy": "2025-10-28T01:32:36.215552Z",
     "iopub.status.idle": "2025-10-28T01:32:36.219286Z",
     "shell.execute_reply": "2025-10-28T01:32:36.218746Z",
     "shell.execute_reply.started": "2025-10-28T01:32:36.215955Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ban hành', 'Nghị định', 'sửa đổi', 'Luật', 'Đất đai']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('Ban hành Nghị định sửa đổi Luật Đất đai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:32:36.220105Z",
     "iopub.status.busy": "2025-10-28T01:32:36.219879Z",
     "iopub.status.idle": "2025-10-28T01:32:36.357928Z",
     "shell.execute_reply": "2025-10-28T01:32:36.357342Z",
     "shell.execute_reply.started": "2025-10-28T01:32:36.220088Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ban', 'hành', 'Nghị', 'định', 'sửa', 'đổi', 'Luật', 'Đất', 'đai']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('Ban hành Nghị định sửa đổi Luật Đất đai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document type Mask matrix\n",
    "\n",
    "To be used in the Self-attention layer of Encoder, note that although word_tokenize can be applied for Wseg, we have to follow the PhoBERT tokenization formula to maintain the consistency and thus making the mask 1 in both subwords of a Wseg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a tokenized sentence (by Wseg)\n",
    "\n",
    "S = [$w_1, w_2,....w_N$]\n",
    "\n",
    "and a lexicon (defined list of known legal document types)\n",
    "\n",
    "Create a mask matrix $M_L \\in R^{N x N}$\n",
    "\n",
    "$m_ij$ = 1 if S[i:j] matches lexicon phrase\n",
    "\n",
    "= 0 otherwise\n",
    "\n",
    "Also rescale so that non-legal tokens can still attend to other tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:38:00.040068Z",
     "iopub.status.busy": "2025-10-29T14:38:00.039367Z",
     "iopub.status.idle": "2025-10-29T14:38:00.046147Z",
     "shell.execute_reply": "2025-10-29T14:38:00.045333Z",
     "shell.execute_reply.started": "2025-10-29T14:38:00.040042Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_legal_mask(text):\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    n = len(tokens)\n",
    "    M = torch.zeros((n, n), dtype=torch.int)\n",
    "\n",
    "    single = {'luật', 'pháp', 'điều', 'chương', 'khoản', 'mục'}\n",
    "    anchors = {\"nghị\", \"thông\", \"quyết\", \"hiến\", \"luật\", \"pháp\"}\n",
    "    followers = {\"định\", \"quyết\", \"tư\", \"pháp\", \"lệnh\"}\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        # mark single-word types like \"luật\", \"pháp\"\n",
    "        if tok in single:\n",
    "            M[i, i] = 1\n",
    "        # mark legal multiword combos (anchor + follower)\n",
    "        if tok in anchors and i + 1 < n:\n",
    "            if tokens[i + 1] in followers:\n",
    "                M[i, i] = 1\n",
    "                M[i + 1, i + 1] = 1\n",
    "                M[i, i + 1] = 1\n",
    "                M[i + 1, i] = 1\n",
    "    \n",
    "    M = 0.1 + 0.9 * M #rescale so that non-legal token can still attend\n",
    "\n",
    "    return tokens, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:32:36.375866Z",
     "iopub.status.busy": "2025-10-28T01:32:36.375597Z",
     "iopub.status.idle": "2025-10-28T01:32:36.389961Z",
     "shell.execute_reply": "2025-10-28T01:32:36.389423Z",
     "shell.execute_reply.started": "2025-10-28T01:32:36.375846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokens, legal = build_legal_mask('Ban hành Nghị định 23Bi/2312/NĐ-cp sửa đổi bổ sung Luật đất đai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:32:36.390732Z",
     "iopub.status.busy": "2025-10-28T01:32:36.390531Z",
     "iopub.status.idle": "2025-10-28T01:32:36.405128Z",
     "shell.execute_reply": "2025-10-28T01:32:36.404591Z",
     "shell.execute_reply.started": "2025-10-28T01:32:36.390717Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(legal, cmap='Greys', interpolation='nearest')\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual-encoding layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PhoBERT General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:38:04.662024Z",
     "iopub.status.busy": "2025-10-29T14:38:04.661163Z",
     "iopub.status.idle": "2025-10-29T14:38:04.671047Z",
     "shell.execute_reply": "2025-10-29T14:38:04.670287Z",
     "shell.execute_reply.started": "2025-10-29T14:38:04.661990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PhoBertEmbedding(nn.Module):\n",
    "    def __init__(self, model_name=\"vinai/phobert-base\", device=None, freeze=False, max_length = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if freeze:\n",
    "            for p in self.model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def encode(self, texts):\n",
    "        toks = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length = self.max_length)\n",
    "        input_ids = toks[\"input_ids\"].to(self.device)\n",
    "        attention_mask = toks[\"attention_mask\"].to(self.device)\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        # last_hidden_state: (batch, seq_len, hidden)\n",
    "        return outputs.last_hidden_state, attention_mask, toks\n",
    "    \n",
    "    #only different by name but for Module usage\n",
    "    def forward(self, texts):\n",
    "        toks = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length = self.max_length)\n",
    "        input_ids = toks[\"input_ids\"].to(self.device)\n",
    "        attention_mask = toks[\"attention_mask\"].to(self.device)\n",
    "\n",
    "        with torch.set_grad_enabled(not self.model.training):\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "\n",
    "        return outputs.last_hidden_state, attention_mask, toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:32:36.421868Z",
     "iopub.status.busy": "2025-10-28T01:32:36.421654Z",
     "iopub.status.idle": "2025-10-28T01:32:36.437894Z",
     "shell.execute_reply": "2025-10-28T01:32:36.437222Z",
     "shell.execute_reply.started": "2025-10-28T01:32:36.421849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text = 'Ban hành Nghị định sửa đổi Luật Đất đai'\n",
    "\n",
    "phobert = PhoBertEmbedding(freeze=True)\n",
    "embeddings, attention_mask, toks = phobert.encode([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative if PhoBERT is not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:38:07.842351Z",
     "iopub.status.busy": "2025-10-29T14:38:07.842071Z",
     "iopub.status.idle": "2025-10-29T14:38:07.848879Z",
     "shell.execute_reply": "2025-10-29T14:38:07.848086Z",
     "shell.execute_reply.started": "2025-10-29T14:38:07.842329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.cls_token = \"<cls>\"\n",
    "        self.sep_token = \"<sep>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "        self.pad_token_id = 0\n",
    "        self.cls_token_id = 1\n",
    "        self.sep_token_id = 2\n",
    "        self.unk_token_id = 3\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text.split()\n",
    "\n",
    "    def encode(self, text, max_length=256, padding=True, truncation=True, return_tensors=None):\n",
    "        tokens = [self.cls_token] + text.split()[: max_length - 2] + [self.sep_token]\n",
    "        input_ids = list(range(len(tokens)))  # dummy token ids\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        if padding and len(input_ids) < max_length:\n",
    "            pad_len = max_length - len(input_ids)\n",
    "            input_ids += [self.pad_token_id] * pad_len\n",
    "            attention_mask += [0] * pad_len\n",
    "\n",
    "        if return_tensors == \"pt\":\n",
    "            import torch\n",
    "            input_ids = torch.tensor([input_ids])\n",
    "            attention_mask = torch.tensor([attention_mask])\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "    def __call__(self, text, **kwargs):\n",
    "        return self.encode(text, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Toggle use of PhoBERT \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:38:11.148470Z",
     "iopub.status.busy": "2025-10-29T14:38:11.147579Z",
     "iopub.status.idle": "2025-10-29T14:38:11.152482Z",
     "shell.execute_reply": "2025-10-29T14:38:11.151695Z",
     "shell.execute_reply.started": "2025-10-29T14:38:11.148438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_JAX\"] = \"1\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:38:11.693600Z",
     "iopub.status.busy": "2025-10-29T14:38:11.692793Z",
     "iopub.status.idle": "2025-10-29T14:38:11.970417Z",
     "shell.execute_reply": "2025-10-29T14:38:11.969663Z",
     "shell.execute_reply.started": "2025-10-29T14:38:11.693575Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhoBERT enabled. Using PhoBERTEmbedder()\n"
     ]
    }
   ],
   "source": [
    "use_phobert = True\n",
    "\n",
    "if use_phobert:\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base')\n",
    "    print('PhoBERT enabled. Using PhoBERTEmbedder()')\n",
    "else:\n",
    "    tokenizer = SimpleTokenizer()\n",
    "    phobert = None\n",
    "    print(\"PhoBERT disabled. Using SimpleTokenizer().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern Transformer-based models replace the traditional PE technique with using Relative Position. Instead of calculating the actual sinusoidal position of the token, the model encode the distance between Q-K tokens, which brings better information about how words interact with each other in a sentence. \n",
    "\n",
    "For a sequence of Length L: $$rel_{ij} = j - i$$\n",
    "\n",
    "we get a matrix of shape (L,L) showing the relative distance of each token with one other\n",
    "\n",
    "For T5-style Relative Position Bias, this embedding of relative distances gives a scalar bias per head per (i,j) and for each attention head h, we get a bias matrix $B^{(h)} \\in R^{L x L}$\n",
    "\n",
    "This bias is added directly to attention logits before the softmax:\n",
    "\n",
    "$$Attention^{(h)} = softmax(\\frac{QK^T}{\\sqrt{d_k}} + B^{(h)})V$$\n",
    "\n",
    "and also be combined with the Lexicon masking \n",
    "\n",
    "$$Attention^{(h)} = softmax(M_D * (\\frac{QK^T}{\\sqrt{d_k}} + B^{(h)}))V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### POStag processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a batch of n sentence, which with l length [a,b,c,....] and a total of K possible POS tag, we create a relative POStag for each sentence: \n",
    "\n",
    "$$n_{[i,j]} = (pos_i, pos_j)$$\n",
    "\n",
    "$$n_{[i, j]} = pos_i * K + pos_j$$\n",
    "\n",
    "and We can get the embedding of this representation P = (batch_size, n_heads, l, l)\n",
    "\n",
    "Add with Postag \n",
    "\n",
    "$$Attention_{Multihead Self}^{(h)} = softmax(\\frac{QK^T}{\\sqrt{d_k}} + B^{(h)} + M_L+ P)V$$\n",
    "\n",
    "Where: \n",
    "\n",
    "* Q: Query matrix (current token) (l, l)\n",
    "\n",
    "* K: Key matrix (each other token in the sentence) (l, l)\n",
    "\n",
    "* V: Value matrix (value of each other token) (l, l)\n",
    "\n",
    "* $d_k$: head_dim (= hidden dim/ n_head)\n",
    "\n",
    "* $B^{(h)}$: Relative Positional Encoding (n_heads, l, l)\n",
    "\n",
    "* $M_L$: Lexicon Mask (l, l)\n",
    "\n",
    "* $P$: Relative POStag encoding (B, n_heads, l, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:38:16.790732Z",
     "iopub.status.busy": "2025-10-29T14:38:16.790142Z",
     "iopub.status.idle": "2025-10-29T14:38:16.798008Z",
     "shell.execute_reply": "2025-10-29T14:38:16.797097Z",
     "shell.execute_reply.started": "2025-10-29T14:38:16.790708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Learned absolute positions\n",
    "\n",
    "class RelativePositionBias(nn.Module):\n",
    "    \"\"\"\n",
    "    biases that are added to attention logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_distance, n_heads):\n",
    "        super().__init__()\n",
    "        self.max_distance = max_distance\n",
    "        self.n_heads = n_heads\n",
    "        # relative distances range from -max_distance..+max_distance -> 2*max_distance+1 buckets for exmple -8->8 to 0->16\n",
    "        self.rel_emb = nn.Embedding(2 * max_distance + 1, n_heads)\n",
    "\n",
    "    def forward(self, seq_len, device=None):\n",
    "        device = device or next(self.rel_emb.parameters()).device\n",
    "        # compute matrix of relative distances j - i\n",
    "        idxs = torch.arange(seq_len, device=device)\n",
    "        rel = idxs.unsqueeze(0) - idxs.unsqueeze(1)  # (seq, seq) with relative distances\n",
    "        clipped = rel.clamp(-self.max_distance, self.max_distance) + self.max_distance #clip the values to positive range\n",
    "        biases = self.rel_emb(clipped).permute(2, 0, 1)  # (n_heads, seq, seq) embedding for trainable\n",
    "        return biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:39:43.028189Z",
     "iopub.status.busy": "2025-10-29T14:39:43.027915Z",
     "iopub.status.idle": "2025-10-29T14:39:43.034349Z",
     "shell.execute_reply": "2025-10-29T14:39:43.033466Z",
     "shell.execute_reply.started": "2025-10-29T14:39:43.028171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Traditional sinusoidal positions\n",
    "\n",
    "import math\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        # a long enough matrix of position encodings\n",
    "        position = torch.arange(max_len).unsqueeze(1) #(max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2) * (-math.log(10000.0) / dim))\n",
    "\n",
    "        #sine/cosine positional encodings\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, dim)\n",
    "\n",
    "        # Register as buffer (not a parameter, not updated by optimizer)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, seq_len: int, device=None):\n",
    "        \"\"\"\n",
    "        Returns positional encodings for a sequence of length seq_len.\n",
    "        Output shape: (1, seq_len, dim)\n",
    "        \"\"\"\n",
    "        device = device or self.pe.device\n",
    "        return self.pe[:, :seq_len].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:32:36.831440Z",
     "iopub.status.busy": "2025-10-28T01:32:36.830753Z",
     "iopub.status.idle": "2025-10-28T01:32:36.849971Z",
     "shell.execute_reply": "2025-10-28T01:32:36.849261Z",
     "shell.execute_reply.started": "2025-10-28T01:32:36.831421Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Postag processing\n",
    "\n",
    "class POSTag(nn.Module):\n",
    "    def __init__(self, n_postags, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_postags = int(n_postags)   \n",
    "        self.n_heads = n_heads\n",
    "        self.bias_table = nn.Embedding(self.n_postags * self.n_postags, n_heads)\n",
    "\n",
    "    def forward(self, postag_ids):\n",
    "        postag_ids = postag_ids.long()\n",
    "\n",
    "        B, L = postag_ids.shape\n",
    "        device = postag_ids.device             \n",
    "        self.bias_table = self.bias_table.to(device)\n",
    "        tag_i = postag_ids.unsqueeze(2).expand(B, L, L)\n",
    "        tag_j = postag_ids.unsqueeze(1).expand(B, L, L)\n",
    "\n",
    "        pair_index = tag_i * self.n_postags + tag_j\n",
    "        pair_index = pair_index.long()  \n",
    "\n",
    "        bias = self.bias_table(pair_index)  # [B, L, L, n_heads]\n",
    "        bias = bias.permute(0, 3, 1, 2).contiguous()\n",
    "        return bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-head Self-Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:42:49.187700Z",
     "iopub.status.busy": "2025-10-29T14:42:49.187006Z",
     "iopub.status.idle": "2025-10-29T14:42:49.198346Z",
     "shell.execute_reply": "2025-10-29T14:42:49.197577Z",
     "shell.execute_reply.started": "2025-10-29T14:42:49.187675Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dropout=0.1, pre_ln=True, use_rel_pos=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = dim // n_heads\n",
    "        self.scale = self.d_k ** 0.5\n",
    "        self.pre_ln = pre_ln\n",
    "        self.use_rel_pos = use_rel_pos  \n",
    "\n",
    "        self.W_q = nn.Linear(dim, dim)\n",
    "        self.W_k = nn.Linear(dim, dim)\n",
    "        self.W_v = nn.Linear(dim, dim)\n",
    "        self.W_o = nn.Linear(dim, dim)\n",
    "        self.norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query,\n",
    "        context=None,\n",
    "        pos_bias=None,     # expected from RelativePositionBias\n",
    "        sinusoidal_pe=None, # expected from SinusoidalPositionalEncoding\n",
    "        postag_bias=None,\n",
    "        mask=None,\n",
    "        lex_mask=None,\n",
    "        multiplicative=False,\n",
    "    ):\n",
    "        if context is None:\n",
    "            context = query  # self-attention\n",
    "        residual = query\n",
    "\n",
    "        if self.pre_ln:\n",
    "            query = self.norm(query)\n",
    "\n",
    "        if not self.use_rel_pos and sinusoidal_pe is not None:\n",
    "            seq_len = query.size(1)\n",
    "            query = query + sinusoidal_pe[:, :seq_len, :].to(query.device)\n",
    "            context = context + sinusoidal_pe[:, :seq_len, :].to(context.device)\n",
    "\n",
    "        B, L, _ = query.size()\n",
    "        _, S, _ = context.size()\n",
    "\n",
    "        Q = self.W_q(query).view(B, L, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(context).view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(context).view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        attn_logits = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        if self.use_rel_pos and pos_bias is not None:\n",
    "            attn_logits = attn_logits + pos_bias.unsqueeze(0)\n",
    "\n",
    "        if postag_bias is not None:\n",
    "            B2, H2, Lp, _ = postag_bias.shape\n",
    "            _, H, L, S = attn_logits.shape\n",
    "            if Lp != L:\n",
    "                if Lp < L:\n",
    "                    pad_len = L - Lp\n",
    "                    postag_bias = F.pad(postag_bias, (0, pad_len, 0, pad_len), value=0.0)\n",
    "                else:\n",
    "                    postag_bias = postag_bias[:, :, :L, :L]\n",
    "            attn_logits = attn_logits + postag_bias\n",
    "\n",
    "\n",
    "        # Lexicon masking\n",
    "        if lex_mask is not None:\n",
    "            lex_mask = lex_mask.unsqueeze(1).unsqueeze(2)\n",
    "            attn_logits = attn_logits * lex_mask if multiplicative else attn_logits + lex_mask\n",
    "\n",
    "\n",
    "        # Attention masking\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            attn_logits = attn_logits.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = F.softmax(attn_logits, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, V).transpose(1, 2).contiguous().view(B, L, self.dim)\n",
    "        out = self.W_o(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out + residual\n",
    "        if not self.pre_ln:\n",
    "            out = self.norm(out)\n",
    "\n",
    "        return out, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feed-Forward NN with Residual Connection and LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that modern models (T5, DeBERTa, GPT) apply Pre-LN which is LayerNorm before running the first linear combination and also apply 2 dropout after each linear combination to help regularize the parameters to avoid overfitting, useful in large deep models\n",
    "\n",
    "Also modern models utilize the GeLU activation instead of traditional ReLU in the MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GeLU: Gaussian Error Linear Unit has the mathematical definition: \n",
    "\n",
    "$$GeLU(x) = x.\\Theta(x)$$\n",
    "\n",
    "where $\\Theta(x)$ is the cumulative distribution function (CDF) of a standard normal distribution \n",
    "\n",
    "$$GeLU(x) = 0.5x(1 + erf(\\frac{x}{\\sqrt{2}}))$$\n",
    "\n",
    "Instead of making hard decision whether x > 0 in ReLU, GeLU makes a probabilistic decision based on how large x is, this makes the gradient smoother than ReLU and allows smoother transition while partially keeping small negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:32:36.867250Z",
     "iopub.status.busy": "2025-10-28T01:32:36.867092Z",
     "iopub.status.idle": "2025-10-28T01:32:36.889016Z",
     "shell.execute_reply": "2025-10-28T01:32:36.888425Z",
     "shell.execute_reply.started": "2025-10-28T01:32:36.867238Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.linspace(-5, 5, 200)\n",
    "relu = F.relu(x)\n",
    "gelu = F.gelu(x)\n",
    "\n",
    "plt.plot(x, relu, label='ReLU')\n",
    "plt.plot(x, gelu, label='GELU')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title(\"ReLU vs GELU\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:42:54.096237Z",
     "iopub.status.busy": "2025-10-29T14:42:54.095973Z",
     "iopub.status.idle": "2025-10-29T14:42:54.101625Z",
     "shell.execute_reply": "2025-10-29T14:42:54.101009Z",
     "shell.execute_reply.started": "2025-10-29T14:42:54.096217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or 4 * dim  # default expansion that hidden dim is 4 x model_dim\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-LN + MLP + Residual\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return residual + x #Skip connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:44:25.964475Z",
     "iopub.status.busy": "2025-10-29T14:44:25.963729Z",
     "iopub.status.idle": "2025-10-29T14:44:25.972116Z",
     "shell.execute_reply": "2025-10-29T14:44:25.971312Z",
     "shell.execute_reply.started": "2025-10-29T14:44:25.964451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_heads,\n",
    "        dropout=0.1,\n",
    "        ff_hidden_dim=None,\n",
    "        max_distance=256,\n",
    "        pre_ln=True,\n",
    "        num_postags=None,\n",
    "        use_rel_pos=True,    \n",
    "        max_seq_len=5000         \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_rel_pos = use_rel_pos\n",
    "\n",
    "        # Attention + Feedforward\n",
    "        self.attn = MultiHeadAttention(\n",
    "            dim,\n",
    "            n_heads,\n",
    "            dropout=dropout,\n",
    "            pre_ln=pre_ln,\n",
    "            use_rel_pos=use_rel_pos   # pass toggle\n",
    "        )\n",
    "        self.ff = FeedForward(dim, hidden_dim=ff_hidden_dim, dropout=dropout)\n",
    "\n",
    "        # Positional representations\n",
    "        if use_rel_pos:\n",
    "            self.pos_module = RelativePositionBias(max_distance=max_distance, n_heads=n_heads)\n",
    "        else:\n",
    "            self.pos_module = SinusoidalPositionalEncoding(dim, max_len=max_seq_len)\n",
    "\n",
    "        # Optional POS-tag bias\n",
    "        self.postag_bias = POSTag(n_postags=num_postags, n_heads=n_heads) if num_postags is not None else None\n",
    "\n",
    "    def forward(self, x, postag_ids=None, lex_mask=None):\n",
    "        seq_len = x.size(1)\n",
    "        device = x.device\n",
    "\n",
    "        # Get positional representation depending on mode\n",
    "\n",
    "        if self.use_rel_pos:\n",
    "            pos_rep = self.pos_module(seq_len, device=device)  # (n_heads, L, L)\n",
    "            pos_kwargs = {\"pos_bias\": pos_rep, \"sinusoidal_pe\": None}\n",
    "        else:\n",
    "            pos_rep = self.pos_module(seq_len, device=device)  # (1, L, dim)\n",
    "            pos_kwargs = {\"pos_bias\": None, \"sinusoidal_pe\": pos_rep}\n",
    "\n",
    "\n",
    "        # Compute POS-tag bias if applicable\n",
    "        postag_bias = None\n",
    "        if self.postag_bias is not None and postag_ids is not None:\n",
    "            postag_bias = self.postag_bias(postag_ids)  # (B, n_heads, L, L)\n",
    "            postag_bias = postag_bias[..., :seq_len, :seq_len]\n",
    "\n",
    "        #  Self-Attention\n",
    "        x, _ = self.attn(\n",
    "            query=x,\n",
    "            context=None,\n",
    "            postag_bias=postag_bias,\n",
    "            lex_mask=lex_mask,\n",
    "            **pos_kwargs   # dynamically pass correct positional arg\n",
    "        )\n",
    "\n",
    "        #  Feed Forward\n",
    "\n",
    "        x = self.ff(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:46:50.592976Z",
     "iopub.status.busy": "2025-10-29T14:46:50.592494Z",
     "iopub.status.idle": "2025-10-29T14:46:50.598820Z",
     "shell.execute_reply": "2025-10-29T14:46:50.598100Z",
     "shell.execute_reply.started": "2025-10-29T14:46:50.592954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class StackedEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim=768,\n",
    "                 n_heads=12,\n",
    "                 ff_hidden_dim=2048,\n",
    "                 dropout=0.1,\n",
    "                 max_distance=128,\n",
    "                 pre_ln=True,\n",
    "                 num_postags=None,\n",
    "                 use_rel_pos = True,\n",
    "                 num_layers=6):  # number of encoder blocks\n",
    "        super().__init__()\n",
    "\n",
    "        # Stack of independent TransformerEncoder blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoder(\n",
    "                dim=dim,\n",
    "                n_heads=n_heads,\n",
    "                dropout=dropout,\n",
    "                ff_hidden_dim=ff_hidden_dim,\n",
    "                max_distance=max_distance,\n",
    "                pre_ln=pre_ln,\n",
    "                num_postags=num_postags,\n",
    "                use_rel_pos = use_rel_pos\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final normalization — important if using pre-LN blocks (like your TransformerEncoder)\n",
    "        self.final_norm = nn.LayerNorm(dim) if pre_ln else nn.Identity()\n",
    "\n",
    "    def forward(self, x, postag_ids=None, lex_mask=None, output_hidden_states=False):\n",
    "\n",
    "        hidden_states = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, postag_ids=postag_ids, lex_mask=lex_mask)\n",
    "            if output_hidden_states:\n",
    "                hidden_states.append(x)\n",
    "\n",
    "        # Apply normalization after all layers (for pre-LN Transformer)\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            return x, hidden_states\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:46:58.296456Z",
     "iopub.status.busy": "2025-10-29T14:46:58.295893Z",
     "iopub.status.idle": "2025-10-29T14:46:58.303805Z",
     "shell.execute_reply": "2025-10-29T14:46:58.303031Z",
     "shell.execute_reply.started": "2025-10-29T14:46:58.296433Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CombinedEmbedding(nn.Module):\n",
    "    def __init__(self, phobert=None, transformer_encoder=None, alpha=0.5, use_phobert=False, max_len=256):\n",
    "        super().__init__()\n",
    "        self.use_phobert = use_phobert\n",
    "        self.phobert = phobert if use_phobert else None\n",
    "        self.encoder = transformer_encoder\n",
    "        self.alpha = nn.Parameter(torch.tensor(alpha))  # learnable interpolation factor\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, texts, input_ids=None):\n",
    "        batch_size = len(texts)\n",
    "        device = next(self.encoder.parameters()).device\n",
    "        seq_len = self.max_len\n",
    "        hidden_dim = self.encoder.input_dim if hasattr(self.encoder, \"input_dim\") else 768\n",
    "\n",
    "        if self.use_phobert and self.phobert is not None:\n",
    "            pho_hidden, attn_mask, toks = self.phobert.encode(texts)\n",
    "            # truncate/pad pho_hidden if needed\n",
    "            if pho_hidden.size(1) > seq_len:\n",
    "                pho_hidden = pho_hidden[:, :seq_len, :]\n",
    "                attn_mask = attn_mask[:, :seq_len]\n",
    "            elif pho_hidden.size(1) < seq_len:\n",
    "                pad_len = seq_len - pho_hidden.size(1)\n",
    "                pho_hidden = torch.cat([pho_hidden, torch.zeros(batch_size, pad_len, hidden_dim, device=device)], dim=1)\n",
    "                attn_mask = torch.cat([attn_mask, torch.zeros(batch_size, pad_len, device=device)], dim=1)\n",
    "        else:\n",
    "            pho_hidden = torch.zeros(batch_size, seq_len, hidden_dim, device=device)\n",
    "            attn_mask = torch.ones(batch_size, seq_len, device=device)\n",
    "            toks = None\n",
    "\n",
    "        trans_hidden = self.encoder(pho_hidden, attn_mask)\n",
    "        alpha = torch.clamp(self.alpha, 0.0, 1.0)\n",
    "\n",
    "        if not self.use_phobert:\n",
    "            return trans_hidden, attn_mask, toks\n",
    "\n",
    "        combined = alpha * pho_hidden + (1 - alpha) * trans_hidden\n",
    "        return combined, attn_mask, toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:47:44.200364Z",
     "iopub.status.busy": "2025-10-29T14:47:44.199475Z",
     "iopub.status.idle": "2025-10-29T14:47:45.740061Z",
     "shell.execute_reply": "2025-10-29T14:47:45.739246Z",
     "shell.execute_reply.started": "2025-10-29T14:47:44.200339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "if use_phobert == True:\n",
    "    phobert = PhoBertEmbedding(freeze=True)\n",
    "\n",
    "if use_phobert == False:\n",
    "    phobert = None\n",
    "    \n",
    "encoder = StackedEncoder(dim=768, n_heads=12, ff_hidden_dim=2048, dropout=0.1, num_layers=6, max_distance = 128, use_rel_pos = False)\n",
    "\n",
    "combined_model = CombinedEmbedding(None, encoder, alpha=0.5)\n",
    "\n",
    "texts = [\n",
    "    \"Doanh nghiệp có thu nhập chịu thuế quy định tại Điều 3 của Luật này phải nộp thuế.\",\n",
    "    \"Quyết định Căn cứ Nghị định số 55/2025/NĐ-CP ngày 02 tháng 3 năm 2025 của Chính phủ quy định chức năng, nhiệm vụ, quyền hạn và cơ cấu tổ chức của Bộ Khoa học và Công nghệ\"\n",
    "]\n",
    "\n",
    "combined_output, attn_mask, toks = combined_model(texts)\n",
    "print(combined_output.shape)  # (batch, seq_len, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional BiLSTM to catch the semantic structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:47:48.690538Z",
     "iopub.status.busy": "2025-10-29T14:47:48.690081Z",
     "iopub.status.idle": "2025-10-29T14:47:48.695093Z",
     "shell.execute_reply": "2025-10-29T14:47:48.694364Z",
     "shell.execute_reply.started": "2025-10-29T14:47:48.690512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bilstm = nn.LSTM(input_dim, hidden_dim // 2,\n",
    "                              num_layers=num_layers,\n",
    "                              dropout=dropout,\n",
    "                              bidirectional=True,\n",
    "                              batch_first=True)\n",
    "    def forward(self, x):\n",
    "        output, _ = self.bilstm(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Head (for Triplet prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:47:49.450474Z",
     "iopub.status.busy": "2025-10-29T14:47:49.450217Z",
     "iopub.status.idle": "2025-10-29T14:47:49.456213Z",
     "shell.execute_reply": "2025-10-29T14:47:49.455474Z",
     "shell.execute_reply.started": "2025-10-29T14:47:49.450457Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Indenpendent Prediction\n",
    "\n",
    "class DecoderHeads(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_relations):\n",
    "        super().__init__()\n",
    "        self.self_root = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "        self.relation = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, num_relations)\n",
    "        )\n",
    "\n",
    "        # span for start and end indices\n",
    "        self.start = nn.Linear(hidden_dim, 1)\n",
    "        self.end = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pool once for classification\n",
    "        pooled = x.mean(dim=1)  #(batch_size, hidden_dim)\n",
    "\n",
    "        self_root_logits = self.self_root(pooled).squeeze(-1)\n",
    "        relation_logits = self.relation(pooled)\n",
    "\n",
    "        # start and end idx\n",
    "        start_logits = self.start(x).squeeze(-1)\n",
    "        end_logits = self.end(x).squeeze(-1)\n",
    "\n",
    "        return self_root_logits, relation_logits, start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:47:51.779620Z",
     "iopub.status.busy": "2025-10-29T14:47:51.779144Z",
     "iopub.status.idle": "2025-10-29T14:47:51.788125Z",
     "shell.execute_reply": "2025-10-29T14:47:51.787397Z",
     "shell.execute_reply.started": "2025-10-29T14:47:51.779601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Recursive prediction\n",
    "\n",
    "class DecoderHeadCopy(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_relations):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.root = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "        self.relation = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_relations)\n",
    "        )\n",
    "\n",
    "        self.start_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 + num_relations, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.end_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 3 + num_relations, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (B, L, H)\n",
    "        \"\"\"\n",
    "        if x.dim() == 4:\n",
    "            x = x.squeeze(1)  # handle (B, 1, L, H)\n",
    "\n",
    "        # root\n",
    "        root_logits = self.root(x).squeeze(-1)  # (B, L)\n",
    "        root_probs = torch.sigmoid(root_logits)\n",
    "        root_vec = (x * root_probs.unsqueeze(-1)).sum(dim=1) / root_probs.sum(dim=1, keepdim=True).clamp(min=1.0)  # (B, H)\n",
    "\n",
    "        # relation based on root\n",
    "        root_context = root_vec.unsqueeze(1).expand(-1, x.size(1), -1)  # (B, L, H)\n",
    "        rel_input = torch.cat([x, root_context], dim=-1)  # (B, L, 2H)\n",
    "        rel_logits = self.relation(rel_input).mean(dim=1)  # (B, num_rel)\n",
    "        rel_probs = F.softmax(rel_logits, dim=-1)\n",
    "\n",
    "        # start idx based on root and relation\n",
    "        rel_context = rel_probs.unsqueeze(1).expand(-1, x.size(1), -1)  # (B, L, num_rel)\n",
    "        start_input = torch.cat([x, root_context, rel_context], dim=-1)\n",
    "        start_logits = self.start_head(start_input).squeeze(-1)  # (B, L)\n",
    "        start_probs = F.softmax(start_logits, dim=-1)\n",
    "        start_vec = (x * start_probs.unsqueeze(-1)).sum(dim=1)  # (B, H)\n",
    "\n",
    "        # end idx based on root, relation and start idx\n",
    "        start_context = start_vec.unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "        end_input = torch.cat([x, root_context, rel_context, start_context], dim=-1)\n",
    "        end_logits = self.end_head(end_input).squeeze(-1)  # (B, L)\n",
    "\n",
    "        return root_logits, rel_logits, start_logits, end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:47:54.727241Z",
     "iopub.status.busy": "2025-10-29T14:47:54.726710Z",
     "iopub.status.idle": "2025-10-29T14:47:54.733658Z",
     "shell.execute_reply": "2025-10-29T14:47:54.732872Z",
     "shell.execute_reply.started": "2025-10-29T14:47:54.727218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderBody(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim=768,\n",
    "                 num_heads=8,\n",
    "                 ffn_dim=2048,\n",
    "                 dropout=0.1,\n",
    "                 pre_ln=True):\n",
    "        super().__init__()\n",
    "        self.cross_attn = MultiHeadAttention(hidden_dim, num_heads, dropout=dropout, pre_ln=pre_ln)\n",
    "        self.bilstm = BiLSTMEncoder(hidden_dim, hidden_dim)\n",
    "        self.ffn = FeedForward(hidden_dim, ffn_dim)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, decoder_state, encoder_output, mask=None):\n",
    "        # Project decoder_state to match encoder_output dim if needed\n",
    "        if decoder_state.size(-1) != encoder_output.size(-1):\n",
    "            project_to_dim = nn.Linear(decoder_state.size(-1), encoder_output.size([-1])).to(decoder_state.device)\n",
    "            decoder_state = project_to_dim(decoder_state)\n",
    "\n",
    "        # Ensure shape [B, L, D]\n",
    "        if decoder_state.dim() == 2:\n",
    "            decoder_state = decoder_state.unsqueeze(-1).repeat(1, 1, encoder_output.size(-1))\n",
    "\n",
    "        # ---- Cross-Attention ----\n",
    "        cross_out, _ = self.cross_attn(\n",
    "            query=decoder_state,\n",
    "            context=encoder_output,\n",
    "            mask=mask\n",
    "        )\n",
    "        x = self.norm(cross_out + decoder_state)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # ---- BiLSTM + FFN ----\n",
    "        x = self.bilstm(x)\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        return x  # [B, L, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:47:58.153173Z",
     "iopub.status.busy": "2025-10-29T14:47:58.152944Z",
     "iopub.status.idle": "2025-10-29T14:47:58.159296Z",
     "shell.execute_reply": "2025-10-29T14:47:58.158580Z",
     "shell.execute_reply.started": "2025-10-29T14:47:58.153157Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class StackedDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim=768,\n",
    "                 num_heads=8,\n",
    "                 num_relations=10,\n",
    "                 ffn_dim=2048,\n",
    "                 dropout=0.1,\n",
    "                 pre_ln=True,\n",
    "                 num_layers=6,\n",
    "                 recursive=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Stack of N DecoderBody blocks (no heads inside)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBody(\n",
    "                hidden_dim=hidden_dim,\n",
    "                num_heads=num_heads,\n",
    "                ffn_dim=ffn_dim,\n",
    "                dropout=dropout,\n",
    "                pre_ln=pre_ln\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final normalization after stacked layers\n",
    "        self.final_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # Shared decoder head\n",
    "        self.head1 = DecoderHeads(hidden_dim, num_relations)\n",
    "        self.head2 = DecoderHeadCopy(hidden_dim, num_relations)\n",
    "        self.recursive = recursive\n",
    "\n",
    "    def forward(self, decoder_state, encoder_output, mask=None):\n",
    "        \"\"\"\n",
    "        decoder_state: [B, L, D] or [B, L]\n",
    "        encoder_output: [B, L, D]\n",
    "        mask: [B, L] optional\n",
    "        \"\"\"\n",
    "\n",
    "        x = decoder_state\n",
    "\n",
    "        # Sequentially pass through each DecoderBody block\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, mask)\n",
    "\n",
    "        # Final normalization\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        # Apply the shared decoder head\n",
    "        if self.recursive:\n",
    "            return self.head2(x)\n",
    "            \n",
    "        return self.head1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:00.768391Z",
     "iopub.status.busy": "2025-10-29T14:48:00.767671Z",
     "iopub.status.idle": "2025-10-29T14:48:00.774597Z",
     "shell.execute_reply": "2025-10-29T14:48:00.773659Z",
     "shell.execute_reply.started": "2025-10-29T14:48:00.768366Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 phobert_embedder=None,\n",
    "                 encoder=None,\n",
    "                 decoder=None,\n",
    "                 alpha=0.5,\n",
    "                 use_phobert=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_phobert = use_phobert\n",
    "        self.embedding = phobert_embedder  \n",
    "        self.encoder = encoder              # StackedEncoder\n",
    "        self.decoder = decoder              # StackedDecoder\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, texts, postag_ids):\n",
    "\n",
    "        combined_output, attn_mask, toks = self.embedding(texts)\n",
    "        # combined_output: (B, L, D)\n",
    "        # attn_mask:      (B, L)\n",
    "\n",
    "        #multi-block encoder\n",
    "        encoded_output = self.encoder(\n",
    "            combined_output,\n",
    "            postag_ids=postag_ids,\n",
    "            lex_mask=attn_mask\n",
    "        )\n",
    "\n",
    "        decoder_output = self.decoder(\n",
    "            decoder_state=encoded_output,\n",
    "            encoder_output=encoded_output,\n",
    "            mask=attn_mask\n",
    "        )\n",
    "\n",
    "        # if StackedDecoder returns logits directly (root, rel, start, end), unpack them here:\n",
    "        if isinstance(decoder_output, tuple) and len(decoder_output) == 4:\n",
    "            root_logits, relation_logits, start_logits, end_logits = decoder_output\n",
    "        else:\n",
    "            # if stacked decoder only returns the last hidden state\n",
    "            root_logits = relation_logits = start_logits = end_logits = None\n",
    "\n",
    "        return {\n",
    "            \"self_root\": root_logits,\n",
    "            \"relation\": relation_logits,\n",
    "            \"start_logits\": start_logits,\n",
    "            \"end_logits\": end_logits,\n",
    "            \"mask\": attn_mask,\n",
    "            \"tokens\": toks\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:05.020211Z",
     "iopub.status.busy": "2025-10-29T14:48:05.019746Z",
     "iopub.status.idle": "2025-10-29T14:48:05.392732Z",
     "shell.execute_reply": "2025-10-29T14:48:05.391901Z",
     "shell.execute_reply.started": "2025-10-29T14:48:05.020189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('D:/Study/Education/Projects/Group_Project/rag_model/model/RE/RE_training_final.csv')\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/traindataset/RE_training_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:09.365679Z",
     "iopub.status.busy": "2025-10-28T01:33:09.365423Z",
     "iopub.status.idle": "2025-10-28T01:33:09.369415Z",
     "shell.execute_reply": "2025-10-28T01:33:09.368705Z",
     "shell.execute_reply.started": "2025-10-28T01:33:09.365657Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# #Process Postagging\n",
    "\n",
    "# import ast\n",
    "# df['postag'] = df['postag'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# unique_tags = sorted({tag for tags in df['postag'] for tag in tags})\n",
    "\n",
    "# pos2idx = {tag: idx for idx, tag in enumerate(unique_tags, start=1)}  # start=1 to reserve 0 for padding\n",
    "# idx2pos = {idx: tag for tag, idx in pos2idx.items()}\n",
    "\n",
    "# df['postag_idx'] = df['postag'].apply(lambda tags: [pos2idx[tag] for tag in tags])\n",
    "\n",
    "# print(pos2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:08.164518Z",
     "iopub.status.busy": "2025-10-29T14:48:08.163891Z",
     "iopub.status.idle": "2025-10-29T14:48:08.178597Z",
     "shell.execute_reply": "2025-10-29T14:48:08.177890Z",
     "shell.execute_reply.started": "2025-10-29T14:48:08.164498Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Amended': 0, 'Drop': 1, 'Guide': 2, 'Others': 3, 'PartDrop': 4, 'PartReplace': 5, 'Pursuant': 6, 'Reference': 7, 'Replace': 8}\n"
     ]
    }
   ],
   "source": [
    "relations = sorted(df[\"relation\"].unique())\n",
    "relation2id = {rel: idx for idx, rel in enumerate(relations)}\n",
    "id2relation = {v: k for k, v in relation2id.items()}\n",
    "\n",
    "print(relation2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:27.024778Z",
     "iopub.status.busy": "2025-10-29T14:48:27.024499Z",
     "iopub.status.idle": "2025-10-29T14:48:28.947620Z",
     "shell.execute_reply": "2025-10-29T14:48:28.946798Z",
     "shell.execute_reply.started": "2025-10-29T14:48:27.024759Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhoBERT enabled. Using PhoBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 256])\n",
      "torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "### Test with Postag\n",
    "#Require definitions of df first with the existence of postag_ids\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = 'cuda' #when run on local\n",
    "\n",
    "texts = df['input_text'].iloc[:2].tolist()\n",
    "# postags = df['postag'].iloc[:2].tolist()\n",
    "# pos_ids = [torch.tensor(p, dtype=torch.long) for p in df['postag_idx'].iloc[:2]]\n",
    "\n",
    "# pos_ids = pad_sequence(pos_ids, batch_first=True, padding_value=0).long().to(device)\n",
    "\n",
    "if use_phobert:\n",
    "    phobert = PhoBertEmbedding(freeze=True, max_length=256)\n",
    "    phobert.to(device)\n",
    "    print('PhoBERT enabled. Using PhoBERT')\n",
    "else:\n",
    "    phobert = None\n",
    "    print(\"PhoBERT disabled. Using encoder-only embeddings.\")\n",
    "\n",
    "# initialize encoder & decoder \n",
    "encoder = StackedEncoder(\n",
    "    dim=768,\n",
    "    n_heads=8,\n",
    "    ff_hidden_dim=2048,\n",
    "    dropout=0.1,\n",
    "    # num_postags=len(pos2idx),\n",
    "    num_layers=1,\n",
    "    use_rel_pos = False\n",
    ").to(device)\n",
    "\n",
    "decoder = StackedDecoder(\n",
    "    hidden_dim=768,\n",
    "    num_heads=8,\n",
    "    num_relations=10,\n",
    "    num_layers=3,\n",
    "    recursive=False\n",
    ").to(device)\n",
    "\n",
    "\n",
    "embedding = CombinedEmbedding(\n",
    "    phobert=phobert,\n",
    "    transformer_encoder=encoder,\n",
    "    alpha=0.5,\n",
    "    use_phobert=use_phobert\n",
    ")\n",
    "\n",
    "model = Transformer(\n",
    "    phobert_embedder=embedding,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    alpha=0.5,\n",
    "    use_phobert=use_phobert\n",
    ").to(device)\n",
    "\n",
    "outputs = model(texts, postag_ids=None)\n",
    "\n",
    "print(outputs[\"self_root\"].shape)    # torch.Size([B])\n",
    "print(outputs[\"relation\"].shape)     # torch.Size([B, num_relations])\n",
    "print(outputs[\"start_logits\"].shape) # torch.Size([B, seq_len])\n",
    "print(outputs[\"end_logits\"].shape)   # torch.Size([B, seq_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:31.913843Z",
     "iopub.status.busy": "2025-10-29T14:48:31.913578Z",
     "iopub.status.idle": "2025-10-29T14:48:31.920637Z",
     "shell.execute_reply": "2025-10-29T14:48:31.919903Z",
     "shell.execute_reply.started": "2025-10-29T14:48:31.913824Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 172.7M, Trainable: 37.7M\n"
     ]
    }
   ],
   "source": [
    "total = sum(p.numel() for p in model.parameters())\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total params: {total/1e6:.1f}M, Trainable: {trainable/1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.213818Z",
     "iopub.status.busy": "2025-10-28T01:33:12.213587Z",
     "iopub.status.idle": "2025-10-28T01:33:12.258472Z",
     "shell.execute_reply": "2025-10-28T01:33:12.257932Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.213779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df = gs_to_df_pandas('RE_training')\n",
    "\n",
    "# df['input_text'] = df['input_text'].apply(lambda x: x.replace('_', ' '))\n",
    "\n",
    "# df['second_entity'] = df['second_entity'].apply(lambda x: x.replace('_', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.259162Z",
     "iopub.status.busy": "2025-10-28T01:33:12.258992Z",
     "iopub.status.idle": "2025-10-28T01:33:12.274458Z",
     "shell.execute_reply": "2025-10-28T01:33:12.273969Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.259141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.275284Z",
     "iopub.status.busy": "2025-10-28T01:33:12.275056Z",
     "iopub.status.idle": "2025-10-28T01:33:12.291506Z",
     "shell.execute_reply": "2025-10-28T01:33:12.290824Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.275265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def get_word_indices(row):\n",
    "#     # Clean punctuation and normalize spaces\n",
    "#     text = re.sub(r'[^\\w\\s]', '', row['input_text'])\n",
    "#     entity = re.sub(r'[^\\w\\s]', '', row['second_entity'])\n",
    "    \n",
    "#     text_words = text.split()\n",
    "#     entity_words = entity.split()\n",
    "    \n",
    "#     n = len(entity_words)\n",
    "    \n",
    "#     # Find first matching subsequence\n",
    "#     for i in range(len(text_words) - n + 1):\n",
    "#         if text_words[i:i+n] == entity_words:\n",
    "#             return pd.Series({'start_word_idx': i, 'end_word_idx': i + n - 1})\n",
    "    \n",
    "#     # Not found\n",
    "#     return pd.Series({'start_word_idx': -1, 'end_word_idx': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.292492Z",
     "iopub.status.busy": "2025-10-28T01:33:12.292255Z",
     "iopub.status.idle": "2025-10-28T01:33:12.314703Z",
     "shell.execute_reply": "2025-10-28T01:33:12.314173Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.292472Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df[[\"start_idx\", \"end_idx\"]] = df.apply(get_word_indices, axis=1).astype(int)\n",
    "\n",
    "# df['first_entity'] = df['first_entity'].apply(lambda x: 1 if x == 'self' else 0)\n",
    "\n",
    "# df = df.rename(columns={'first_entity': 'self_root'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.315588Z",
     "iopub.status.busy": "2025-10-28T01:33:12.315413Z",
     "iopub.status.idle": "2025-10-28T01:33:12.330169Z",
     "shell.execute_reply": "2025-10-28T01:33:12.329642Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.315575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.331100Z",
     "iopub.status.busy": "2025-10-28T01:33:12.330768Z",
     "iopub.status.idle": "2025-10-28T01:33:12.346450Z",
     "shell.execute_reply": "2025-10-28T01:33:12.345964Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.331079Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# write_df_to_gs(df, 'RE_training_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add extra input feature from text - postagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.347303Z",
     "iopub.status.busy": "2025-10-28T01:33:12.347068Z",
     "iopub.status.idle": "2025-10-28T01:33:12.362953Z",
     "shell.execute_reply": "2025-10-28T01:33:12.362451Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.347280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# postag = df['input_text'].apply(lambda x: annotator.annotate(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.363670Z",
     "iopub.status.busy": "2025-10-28T01:33:12.363455Z",
     "iopub.status.idle": "2025-10-28T01:33:12.378978Z",
     "shell.execute_reply": "2025-10-28T01:33:12.378265Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.363655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def create_pos(text):   \n",
    "#     pos_tagging = []\n",
    "\n",
    "#     pos = annotator.annotate(text)\n",
    "\n",
    "#     for result_item in pos:\n",
    "\n",
    "#         sentence_segments = result_item['sentences'][0]\n",
    "        \n",
    "#         current_sentence_tags = []\n",
    "#         for segment in sentence_segments:\n",
    "#             word_form = segment['form']\n",
    "#             pos_tag = segment['posTag']\n",
    "            \n",
    "#             sub_words = word_form.split('_')\n",
    "#             repeated_tags = [pos_tag] * len(sub_words)\n",
    "#             current_sentence_tags.extend(repeated_tags)\n",
    "#         pos_tagging.append(current_sentence_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.379900Z",
     "iopub.status.busy": "2025-10-28T01:33:12.379640Z",
     "iopub.status.idle": "2025-10-28T01:33:12.394366Z",
     "shell.execute_reply": "2025-10-28T01:33:12.393657Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.379879Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print('Original text:')\n",
    "# print(df['input_text'][0])\n",
    "# print('Postag:')\n",
    "# print(pos_tagging[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.395348Z",
     "iopub.status.busy": "2025-10-28T01:33:12.395143Z",
     "iopub.status.idle": "2025-10-28T01:33:12.410145Z",
     "shell.execute_reply": "2025-10-28T01:33:12.409423Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.395334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df['postag'] = pos_tagging\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset and Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:41.137621Z",
     "iopub.status.busy": "2025-10-29T14:48:41.137355Z",
     "iopub.status.idle": "2025-10-29T14:48:41.145588Z",
     "shell.execute_reply": "2025-10-29T14:48:41.144796Z",
     "shell.execute_reply.started": "2025-10-29T14:48:41.137602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class trainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, relation2id, max_length=256):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.relation2id = relation2id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row[\"input_text\"]\n",
    "\n",
    "        # Tokenize text\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        input_ids = encoded[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze(0)\n",
    "        seq_len = int(attention_mask.sum())\n",
    "\n",
    "        # # POS tagging indices\n",
    "        # postag_idx = row[\"postag_idx\"]\n",
    "        # if not isinstance(postag_idx, list):\n",
    "        #     postag_idx = [0]  # fallback if something went wrong\n",
    "\n",
    "        # postag_tensor = torch.tensor(postag_idx, dtype=torch.long)\n",
    "        # pad/truncate POS tag sequence to match tokenizer length\n",
    "        # if len(postag_tensor) < self.max_length:\n",
    "        #     pad_len = self.max_length - len(postag_tensor)\n",
    "        #     postag_tensor = torch.cat([postag_tensor, torch.zeros(pad_len, dtype=torch.long)])\n",
    "        # else:\n",
    "        #     postag_tensor = postag_tensor[:self.max_length]\n",
    "\n",
    "        # Start/End indices sanity check \n",
    "        start_idx = int(row[\"start_idx\"])\n",
    "        end_idx = int(row[\"end_idx\"])\n",
    "        if start_idx < 0 or end_idx < 0 or start_idx >= seq_len or end_idx >= seq_len:\n",
    "            print(f\"[WARN] invalid span indices at row {idx}: ({start_idx}, {end_idx}) | seq_len={seq_len}\")\n",
    "            start_idx, end_idx = 0, 0  # fallback inside range\n",
    "\n",
    "        labels = {\n",
    "            \"self_root\": torch.tensor(row[\"self_root\"], dtype=torch.float),\n",
    "            \"relation\": torch.tensor(self.relation2id[row[\"relation\"]], dtype=torch.long),\n",
    "            \"start\": torch.tensor(start_idx, dtype=torch.long),\n",
    "            \"end\": torch.tensor(end_idx, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            # \"postag_ids\": postag_tensor,\n",
    "            \"text\": text,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:41.375704Z",
     "iopub.status.busy": "2025-10-29T14:48:41.375433Z",
     "iopub.status.idle": "2025-10-29T14:48:41.395289Z",
     "shell.execute_reply": "2025-10-29T14:48:41.394618Z",
     "shell.execute_reply.started": "2025-10-29T14:48:41.375683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid spans: 0\n",
      "✅ Remaining rows after drop: 32380\n"
     ]
    }
   ],
   "source": [
    "#Drop rows with no traced second entity and invalid entity span (outside of max_length)\n",
    "\n",
    "bad_rows = df[(df[\"start_idx\"] < 0) | (df[\"end_idx\"] < 0)]\n",
    "print(\"Invalid spans:\", len(bad_rows))\n",
    "\n",
    "df = df.drop(bad_rows.index).reset_index(drop=True)\n",
    "df = df[(df[\"end_idx\"] < 256)] #current max length\n",
    "print(\"✅ Remaining rows after drop:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:43.151156Z",
     "iopub.status.busy": "2025-10-29T14:48:43.150889Z",
     "iopub.status.idle": "2025-10-29T14:48:43.167677Z",
     "shell.execute_reply": "2025-10-29T14:48:43.166910Z",
     "shell.execute_reply.started": "2025-10-29T14:48:43.151137Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Căn cứ Hiến pháp nước Cộng hòa xã hội chủ nghĩa Việt Nam;\n",
      "{'self_root': tensor(1.), 'relation': tensor(6), 'start': tensor(2), 'end': tensor(12)}\n"
     ]
    }
   ],
   "source": [
    "dataset = trainDataset(df, tokenizer, relation2id)\n",
    "\n",
    "sample = dataset[0]\n",
    "print(sample[\"text\"])\n",
    "print(sample[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:43.463639Z",
     "iopub.status.busy": "2025-10-29T14:48:43.462906Z",
     "iopub.status.idle": "2025-10-29T14:48:43.471324Z",
     "shell.execute_reply": "2025-10-29T14:48:43.470583Z",
     "shell.execute_reply.started": "2025-10-29T14:48:43.463608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "total_size = len(dataset)\n",
    "test_size = int(0.2 * total_size)\n",
    "train_size = total_size - test_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:45.520090Z",
     "iopub.status.busy": "2025-10-29T14:48:45.519629Z",
     "iopub.status.idle": "2025-10-29T14:48:45.525412Z",
     "shell.execute_reply": "2025-10-29T14:48:45.524207Z",
     "shell.execute_reply.started": "2025-10-29T14:48:45.520046Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches: 3238\n",
      "Number of test batches: 810\n"
     ]
    }
   ],
   "source": [
    "num_batches = len(train_loader)\n",
    "print(f\"Number of train batches: {num_batches}\")\n",
    "\n",
    "num_batches2 = len(test_loader)\n",
    "print(f\"Number of test batches: {num_batches2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-29T15:30:42.918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "criterion_root = torch.nn.BCEWithLogitsLoss()       # for root\n",
    "criterion_relation = torch.nn.CrossEntropyLoss()    # for relation\n",
    "criterion_span = torch.nn.CrossEntropyLoss()        # for start/end positions\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "device = next(model.parameters()).device  # ensures correct device\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        texts = batch[\"text\"]\n",
    "\n",
    "        # Safe conversion for postag_ids\n",
    "        # postag_ids = batch[\"postag_ids\"].to(model.device if hasattr(model, \"device\") else \"cuda\")\n",
    "\n",
    "        outputs = model(texts, postag_ids=None)\n",
    "\n",
    "        # Unpack model outputs\n",
    "        self_root_logits = outputs[\"self_root\"]\n",
    "        relation_logits = outputs[\"relation\"]\n",
    "        start_logits = outputs[\"start_logits\"]\n",
    "        end_logits = outputs[\"end_logits\"]\n",
    "\n",
    "        labels = batch[\"labels\"]\n",
    "        self_root_labels = labels[\"self_root\"].to(self_root_logits.device)\n",
    "        relation_labels = labels[\"relation\"].to(relation_logits.device)\n",
    "        start_labels = labels[\"start\"].to(start_logits.device)\n",
    "        end_labels = labels[\"end\"].to(end_logits.device)\n",
    "\n",
    "        # Compute loss\n",
    "        loss_self_root = criterion_root(self_root_logits.squeeze(), self_root_labels)\n",
    "        loss_relation = criterion_relation(relation_logits, relation_labels)\n",
    "        loss_start = criterion_span(start_logits, start_labels)\n",
    "        loss_end = criterion_span(end_logits, end_labels)\n",
    "\n",
    "        total_batch_loss = loss_self_root + loss_relation + 0.5 * (loss_start + loss_end)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += total_batch_loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), 'state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T05:40:43.253672Z",
     "iopub.status.busy": "2025-10-28T05:40:43.253068Z",
     "iopub.status.idle": "2025-10-28T05:44:27.705905Z",
     "shell.execute_reply": "2025-10-28T05:44:27.705220Z",
     "shell.execute_reply.started": "2025-10-28T05:40:43.253647Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_root:   Accuracy: 0.9969 | Precision: 0.9847 | Recall: 0.9973 | F1: 0.9909\n",
      "relation:    Accuracy: 0.9617 | Macro-F1: 0.8249\n",
      "span         Precision: 0.8379 | Recall: 0.8379 | F1: 0.8379\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "root_preds, root_labels = [], []\n",
    "rel_preds, rel_labels = [], []\n",
    "pred_spans, true_spans = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:   \n",
    "        texts = batch[\"text\"]\n",
    "        outputs = model(texts, postag_ids = None)\n",
    "\n",
    "        # self_root\n",
    "        self_root_logits = outputs[\"self_root\"].squeeze()\n",
    "        self_root_labels = batch[\"labels\"][\"self_root\"].to(self_root_logits.device)\n",
    "        preds_root = (torch.sigmoid(self_root_logits) > 0.5).long()\n",
    "\n",
    "        root_preds.extend(preds_root.cpu().numpy())\n",
    "        root_labels.extend(self_root_labels.cpu().numpy())\n",
    "\n",
    "        # relation\n",
    "        relation_logits = outputs[\"relation\"]\n",
    "        relation_labels = batch[\"labels\"][\"relation\"].to(relation_logits.device)\n",
    "        preds_relation = torch.argmax(relation_logits, dim=1)\n",
    "\n",
    "        rel_preds.extend(preds_relation.cpu().numpy())\n",
    "        rel_labels.extend(relation_labels.cpu().numpy())\n",
    "\n",
    "        # span\n",
    "        start_logits = outputs[\"start_logits\"]\n",
    "        end_logits = outputs[\"end_logits\"]\n",
    "        start_labels = batch[\"labels\"][\"start\"].to(start_logits.device)\n",
    "        end_labels = batch[\"labels\"][\"end\"].to(end_logits.device)\n",
    "\n",
    "        pred_start = torch.argmax(start_logits, dim=1)\n",
    "        pred_end = torch.argmax(end_logits, dim=1)\n",
    "\n",
    "        for s_pred, e_pred, s_true, e_true in zip(pred_start, pred_end, start_labels, end_labels):\n",
    "            pred_spans.append((s_pred.item(), e_pred.item()))\n",
    "            true_spans.append((s_true.item(), e_true.item()))\n",
    "\n",
    "\n",
    "root_precision = precision_score(root_labels, root_preds)\n",
    "root_recall = recall_score(root_labels, root_preds)\n",
    "root_f1 = f1_score(root_labels, root_preds)\n",
    "root_acc = accuracy_score(root_labels, root_preds)\n",
    "\n",
    "rel_acc = accuracy_score(rel_labels, rel_preds)\n",
    "rel_f1 = f1_score(rel_labels, rel_preds, average='macro')\n",
    "\n",
    "correct_spans = sum([1 for p, t in zip(pred_spans, true_spans) if p == t])\n",
    "span_precision = correct_spans / len(pred_spans)\n",
    "span_recall = correct_spans / len(true_spans)\n",
    "span_f1 = 2 * span_precision * span_recall / (span_precision + span_recall + 1e-8)\n",
    "\n",
    "\n",
    "print(f\"self_root:   Accuracy: {root_acc:.4f} | Precision: {root_precision:.4f} | Recall: {root_recall:.4f} | F1: {root_f1:.4f}\")\n",
    "print(f\"relation:    Accuracy: {rel_acc:.4f} | Macro-F1: {rel_f1:.4f}\")\n",
    "print(f\"span         Precision: {span_precision:.4f} | Recall: {span_recall:.4f} | F1: {span_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8544927,
     "sourceId": 13461753,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8583049,
     "sourceId": 13518026,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 478689,
     "modelInstanceId": 462903,
     "sourceId": 615892,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 482913,
     "modelInstanceId": 467089,
     "sourceId": 621071,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
