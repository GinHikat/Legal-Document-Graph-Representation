{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related works Architecture:\n",
    "\n",
    "- Original Transformer\n",
    "\n",
    "- BERT\n",
    "\n",
    "- BART\n",
    "\n",
    "- T5\n",
    "\n",
    "- DeBERTa\n",
    "\n",
    "- GPT-NeoX\n",
    "\n",
    "- PaLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:37:55.680673Z",
     "iopub.status.busy": "2025-10-29T14:37:55.679902Z",
     "iopub.status.idle": "2025-10-29T14:37:55.684507Z",
     "shell.execute_reply": "2025-10-29T14:37:55.683913Z",
     "shell.execute_reply.started": "2025-10-29T14:37:55.680641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from underthesea import word_tokenize\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import sys, os\n",
    "import difflib\n",
    "\n",
    "# project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n",
    "# if project_root not in sys.path:\n",
    "#     sys.path.append(project_root)\n",
    "\n",
    "# from shared_functions.global_functions import *\n",
    "# from shared_functions.gg_sheet import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to keep in mind the wseg will not be used here to match with the PhoBERT tokenization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:32:36.215974Z",
     "iopub.status.busy": "2025-10-28T01:32:36.215552Z",
     "iopub.status.idle": "2025-10-28T01:32:36.219286Z",
     "shell.execute_reply": "2025-10-28T01:32:36.218746Z",
     "shell.execute_reply.started": "2025-10-28T01:32:36.215955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# word_tokenize('Ban hành Nghị định sửa đổi Luật Đất đai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:32:36.220105Z",
     "iopub.status.busy": "2025-10-28T01:32:36.219879Z",
     "iopub.status.idle": "2025-10-28T01:32:36.357928Z",
     "shell.execute_reply": "2025-10-28T01:32:36.357342Z",
     "shell.execute_reply.started": "2025-10-28T01:32:36.220088Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# tokenizer.tokenize('Ban hành Nghị định sửa đổi Luật Đất đai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document type Mask matrix\n",
    "\n",
    "To be used in the Self-attention layer of Encoder, note that although word_tokenize can be applied for Wseg, we have to follow the PhoBERT tokenization formula to maintain the consistency and thus making the mask 1 in both subwords of a Wseg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a tokenized sentence (by Wseg)\n",
    "\n",
    "S = [$w_1, w_2,....w_N$]\n",
    "\n",
    "and a lexicon (defined list of known legal document types)\n",
    "\n",
    "Create a mask matrix $M_L \\in R^{N x N}$\n",
    "\n",
    "$m_ij$ = 1 if S[i:j] matches lexicon phrase\n",
    "\n",
    "= 0 otherwise\n",
    "\n",
    "Also rescale so that non-legal tokens can still attend to other tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:38:00.040068Z",
     "iopub.status.busy": "2025-10-29T14:38:00.039367Z",
     "iopub.status.idle": "2025-10-29T14:38:00.046147Z",
     "shell.execute_reply": "2025-10-29T14:38:00.045333Z",
     "shell.execute_reply.started": "2025-10-29T14:38:00.040042Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_legal_mask(text):\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    n = len(tokens)\n",
    "    M = torch.zeros((n, n), dtype=torch.int)\n",
    "\n",
    "    single = {'luật', 'pháp', 'điều', 'chương', 'khoản', 'mục'}\n",
    "    anchors = {\"nghị\", \"thông\", \"quyết\", \"hiến\", \"luật\", \"pháp\"}\n",
    "    followers = {\"định\", \"quyết\", \"tư\", \"pháp\", \"lệnh\"}\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        # mark single-word types like \"luật\", \"pháp\"\n",
    "        if tok in single:\n",
    "            M[i, i] = 1\n",
    "        # mark legal multiword combos (anchor + follower)\n",
    "        if tok in anchors and i + 1 < n:\n",
    "            if tokens[i + 1] in followers:\n",
    "                M[i, i] = 1\n",
    "                M[i + 1, i + 1] = 1\n",
    "                M[i, i + 1] = 1\n",
    "                M[i + 1, i] = 1\n",
    "    \n",
    "    M = 0.1 + 0.9 * M #rescale so that non-legal token can still attend\n",
    "\n",
    "    return tokens, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:32:36.375866Z",
     "iopub.status.busy": "2025-10-28T01:32:36.375597Z",
     "iopub.status.idle": "2025-10-28T01:32:36.389961Z",
     "shell.execute_reply": "2025-10-28T01:32:36.389423Z",
     "shell.execute_reply.started": "2025-10-28T01:32:36.375846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# tokens, legal = build_legal_mask('Ban hành Nghị định 23Bi/2312/NĐ-cp sửa đổi bổ sung Luật đất đai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:32:36.390732Z",
     "iopub.status.busy": "2025-10-28T01:32:36.390531Z",
     "iopub.status.idle": "2025-10-28T01:32:36.405128Z",
     "shell.execute_reply": "2025-10-28T01:32:36.404591Z",
     "shell.execute_reply.started": "2025-10-28T01:32:36.390717Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(3, 3))\n",
    "# plt.imshow(legal, cmap='Greys', interpolation='nearest')\n",
    "# # plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual-encoding layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PhoBERT General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:38:04.662024Z",
     "iopub.status.busy": "2025-10-29T14:38:04.661163Z",
     "iopub.status.idle": "2025-10-29T14:38:04.671047Z",
     "shell.execute_reply": "2025-10-29T14:38:04.670287Z",
     "shell.execute_reply.started": "2025-10-29T14:38:04.661990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PhoBertEmbedding(nn.Module):\n",
    "    def __init__(self, model_name=\"/kaggle/input/vinai-phobert-base-v1/pytorch/default/1/c1e37c5c86f918761049cef6fa216b4779d0d01d\", device=None, freeze=False, max_length = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if freeze:\n",
    "            for p in self.model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def encode(self, texts):\n",
    "        toks = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length = self.max_length)\n",
    "        input_ids = toks[\"input_ids\"].to(self.device)\n",
    "        attention_mask = toks[\"attention_mask\"].to(self.device)\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        # last_hidden_state: (batch, seq_len, hidden)\n",
    "        return outputs.last_hidden_state, attention_mask, toks\n",
    "    \n",
    "    #only different by name but for Module usage\n",
    "    def forward(self, texts):\n",
    "        toks = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length = self.max_length)\n",
    "        input_ids = toks[\"input_ids\"].to(self.device)\n",
    "        attention_mask = toks[\"attention_mask\"].to(self.device)\n",
    "\n",
    "        with torch.set_grad_enabled(not self.model.training):\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "\n",
    "        return outputs.last_hidden_state, attention_mask, toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:32:36.421868Z",
     "iopub.status.busy": "2025-10-28T01:32:36.421654Z",
     "iopub.status.idle": "2025-10-28T01:32:36.437894Z",
     "shell.execute_reply": "2025-10-28T01:32:36.437222Z",
     "shell.execute_reply.started": "2025-10-28T01:32:36.421849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# text = 'Ban hành Nghị định sửa đổi Luật Đất đai'\n",
    "\n",
    "# phobert = PhoBertEmbedding(freeze=True)\n",
    "# embeddings, attention_mask, toks = phobert.encode([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative if PhoBERT is not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:38:07.842351Z",
     "iopub.status.busy": "2025-10-29T14:38:07.842071Z",
     "iopub.status.idle": "2025-10-29T14:38:07.848879Z",
     "shell.execute_reply": "2025-10-29T14:38:07.848086Z",
     "shell.execute_reply.started": "2025-10-29T14:38:07.842329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.cls_token = \"<cls>\"\n",
    "        self.sep_token = \"<sep>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "        self.pad_token_id = 0\n",
    "        self.cls_token_id = 1\n",
    "        self.sep_token_id = 2\n",
    "        self.unk_token_id = 3\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text.split()\n",
    "\n",
    "    def encode(self, text, max_length=256, padding=True, truncation=True, return_tensors=None):\n",
    "        tokens = [self.cls_token] + text.split()[: max_length - 2] + [self.sep_token]\n",
    "        input_ids = list(range(len(tokens)))  # dummy token ids\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        if padding and len(input_ids) < max_length:\n",
    "            pad_len = max_length - len(input_ids)\n",
    "            input_ids += [self.pad_token_id] * pad_len\n",
    "            attention_mask += [0] * pad_len\n",
    "\n",
    "        if return_tensors == \"pt\":\n",
    "            import torch\n",
    "            input_ids = torch.tensor([input_ids])\n",
    "            attention_mask = torch.tensor([attention_mask])\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "    def __call__(self, text, **kwargs):\n",
    "        return self.encode(text, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Toggle use of PhoBERT \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:38:11.148470Z",
     "iopub.status.busy": "2025-10-29T14:38:11.147579Z",
     "iopub.status.idle": "2025-10-29T14:38:11.152482Z",
     "shell.execute_reply": "2025-10-29T14:38:11.151695Z",
     "shell.execute_reply.started": "2025-10-29T14:38:11.148438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_JAX\"] = \"1\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:38:11.693600Z",
     "iopub.status.busy": "2025-10-29T14:38:11.692793Z",
     "iopub.status.idle": "2025-10-29T14:38:11.970417Z",
     "shell.execute_reply": "2025-10-29T14:38:11.969663Z",
     "shell.execute_reply.started": "2025-10-29T14:38:11.693575Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhoBERT enabled. Using PhoBERTEmbedder()\n"
     ]
    }
   ],
   "source": [
    "use_phobert = True\n",
    "\n",
    "if use_phobert:\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/vinai-phobert-base-v1/pytorch/default/1/c1e37c5c86f918761049cef6fa216b4779d0d01d')\n",
    "    print('PhoBERT enabled. Using PhoBERTEmbedder()')\n",
    "else:\n",
    "    tokenizer = SimpleTokenizer()\n",
    "    phobert = None\n",
    "    print(\"PhoBERT disabled. Using SimpleTokenizer().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern Transformer-based models replace the traditional PE technique with using Relative Position. Instead of calculating the actual sinusoidal position of the token, the model encode the distance between Q-K tokens, which brings better information about how words interact with each other in a sentence. \n",
    "\n",
    "For a sequence of Length L: $$rel_{ij} = j - i$$\n",
    "\n",
    "we get a matrix of shape (L,L) showing the relative distance of each token with one other\n",
    "\n",
    "For T5-style Relative Position Bias, this embedding of relative distances gives a scalar bias per head per (i,j) and for each attention head h, we get a bias matrix $B^{(h)} \\in R^{L x L}$\n",
    "\n",
    "This bias is added directly to attention logits before the softmax:\n",
    "\n",
    "$$Attention^{(h)} = softmax(\\frac{QK^T}{\\sqrt{d_k}} + B^{(h)})V$$\n",
    "\n",
    "and also be combined with the Lexicon masking \n",
    "\n",
    "$$Attention^{(h)} = softmax(M_D * (\\frac{QK^T}{\\sqrt{d_k}} + B^{(h)}))V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### POStag processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a batch of n sentence, which with l length [a,b,c,....] and a total of K possible POS tag, we create a relative POStag for each sentence: \n",
    "\n",
    "$$n_{[i,j]} = (pos_i, pos_j)$$\n",
    "\n",
    "$$n_{[i, j]} = pos_i * K + pos_j$$\n",
    "\n",
    "and We can get the embedding of this representation P = (batch_size, n_heads, l, l)\n",
    "\n",
    "Add with Postag \n",
    "\n",
    "$$Attention_{Multihead Self}^{(h)} = softmax(\\frac{QK^T}{\\sqrt{d_k}} + B^{(h)} + M_L+ P)V$$\n",
    "\n",
    "Where: \n",
    "\n",
    "* Q: Query matrix (current token) (l, l)\n",
    "\n",
    "* K: Key matrix (each other token in the sentence) (l, l)\n",
    "\n",
    "* V: Value matrix (value of each other token) (l, l)\n",
    "\n",
    "* $d_k$: head_dim (= hidden dim/ n_head)\n",
    "\n",
    "* $B^{(h)}$: Relative Positional Encoding (n_heads, l, l)\n",
    "\n",
    "* $M_L$: Lexicon Mask (l, l)\n",
    "\n",
    "* $P$: Relative POStag encoding (B, n_heads, l, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:38:16.790732Z",
     "iopub.status.busy": "2025-10-29T14:38:16.790142Z",
     "iopub.status.idle": "2025-10-29T14:38:16.798008Z",
     "shell.execute_reply": "2025-10-29T14:38:16.797097Z",
     "shell.execute_reply.started": "2025-10-29T14:38:16.790708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Learned absolute positions\n",
    "\n",
    "class RelativePositionBias(nn.Module):\n",
    "    \"\"\"\n",
    "    biases that are added to attention logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_distance, n_heads):\n",
    "        super().__init__()\n",
    "        self.max_distance = max_distance\n",
    "        self.n_heads = n_heads\n",
    "        # relative distances range from -max_distance..+max_distance -> 2*max_distance+1 buckets for exmple -8->8 to 0->16\n",
    "        self.rel_emb = nn.Embedding(2 * max_distance + 1, n_heads)\n",
    "\n",
    "    def forward(self, seq_len, device=None):\n",
    "        device = device or next(self.rel_emb.parameters()).device\n",
    "        # compute matrix of relative distances j - i\n",
    "        idxs = torch.arange(seq_len, device=device)\n",
    "        rel = idxs.unsqueeze(0) - idxs.unsqueeze(1)  # (seq, seq) with relative distances\n",
    "        clipped = rel.clamp(-self.max_distance, self.max_distance) + self.max_distance #clip the values to positive range\n",
    "        biases = self.rel_emb(clipped).permute(2, 0, 1)  # (n_heads, seq, seq) embedding for trainable\n",
    "        return biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:39:43.028189Z",
     "iopub.status.busy": "2025-10-29T14:39:43.027915Z",
     "iopub.status.idle": "2025-10-29T14:39:43.034349Z",
     "shell.execute_reply": "2025-10-29T14:39:43.033466Z",
     "shell.execute_reply.started": "2025-10-29T14:39:43.028171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Traditional sinusoidal positions\n",
    "\n",
    "import math\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        # a long enough matrix of position encodings\n",
    "        position = torch.arange(max_len).unsqueeze(1) #(max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2) * (-math.log(10000.0) / dim))\n",
    "\n",
    "        #sine/cosine positional encodings\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, dim)\n",
    "\n",
    "        # Register as buffer (not a parameter, not updated by optimizer)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, seq_len: int, device=None):\n",
    "        \"\"\"\n",
    "        Returns positional encodings for a sequence of length seq_len.\n",
    "        Output shape: (1, seq_len, dim)\n",
    "        \"\"\"\n",
    "        device = device or self.pe.device\n",
    "        return self.pe[:, :seq_len].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:32:36.831440Z",
     "iopub.status.busy": "2025-10-28T01:32:36.830753Z",
     "iopub.status.idle": "2025-10-28T01:32:36.849971Z",
     "shell.execute_reply": "2025-10-28T01:32:36.849261Z",
     "shell.execute_reply.started": "2025-10-28T01:32:36.831421Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Postag processing\n",
    "\n",
    "class POSTag(nn.Module):\n",
    "    def __init__(self, n_postags, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_postags = int(n_postags)   \n",
    "        self.n_heads = n_heads\n",
    "        self.bias_table = nn.Embedding(self.n_postags * self.n_postags, n_heads)\n",
    "\n",
    "    def forward(self, postag_ids):\n",
    "        postag_ids = postag_ids.long()\n",
    "\n",
    "        B, L = postag_ids.shape\n",
    "        device = postag_ids.device             \n",
    "        self.bias_table = self.bias_table.to(device)\n",
    "        tag_i = postag_ids.unsqueeze(2).expand(B, L, L)\n",
    "        tag_j = postag_ids.unsqueeze(1).expand(B, L, L)\n",
    "\n",
    "        pair_index = tag_i * self.n_postags + tag_j\n",
    "        pair_index = pair_index.long()  \n",
    "\n",
    "        bias = self.bias_table(pair_index)  # [B, L, L, n_heads]\n",
    "        bias = bias.permute(0, 3, 1, 2).contiguous()\n",
    "        return bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-head Self-Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:42:49.187700Z",
     "iopub.status.busy": "2025-10-29T14:42:49.187006Z",
     "iopub.status.idle": "2025-10-29T14:42:49.198346Z",
     "shell.execute_reply": "2025-10-29T14:42:49.197577Z",
     "shell.execute_reply.started": "2025-10-29T14:42:49.187675Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dropout=0.1, pre_ln=True, use_rel_pos=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = dim // n_heads\n",
    "        self.scale = self.d_k ** 0.5\n",
    "        self.pre_ln = pre_ln\n",
    "        self.use_rel_pos = use_rel_pos  \n",
    "\n",
    "        self.W_q = nn.Linear(dim, dim)\n",
    "        self.W_k = nn.Linear(dim, dim)\n",
    "        self.W_v = nn.Linear(dim, dim)\n",
    "        self.W_o = nn.Linear(dim, dim)\n",
    "        self.norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query,\n",
    "        context=None,\n",
    "        pos_bias=None,     # expected from RelativePositionBias\n",
    "        sinusoidal_pe=None, # expected from SinusoidalPositionalEncoding\n",
    "        postag_bias=None,\n",
    "        mask=None,\n",
    "        lex_mask=None,\n",
    "        multiplicative=False,\n",
    "    ):\n",
    "        if context is None:\n",
    "            context = query  # self-attention\n",
    "        residual = query\n",
    "\n",
    "        if self.pre_ln:\n",
    "            query = self.norm(query)\n",
    "\n",
    "        if not self.use_rel_pos and sinusoidal_pe is not None:\n",
    "            seq_len = query.size(1)\n",
    "            query = query + sinusoidal_pe[:, :seq_len, :].to(query.device)\n",
    "            context = context + sinusoidal_pe[:, :seq_len, :].to(context.device)\n",
    "\n",
    "        B, L, _ = query.size()\n",
    "        _, S, _ = context.size()\n",
    "\n",
    "        Q = self.W_q(query).view(B, L, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(context).view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(context).view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        attn_logits = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        if self.use_rel_pos and pos_bias is not None:\n",
    "            attn_logits = attn_logits + pos_bias.unsqueeze(0)\n",
    "\n",
    "        if postag_bias is not None:\n",
    "            B2, H2, Lp, _ = postag_bias.shape\n",
    "            _, H, L, S = attn_logits.shape\n",
    "            if Lp != L:\n",
    "                if Lp < L:\n",
    "                    pad_len = L - Lp\n",
    "                    postag_bias = F.pad(postag_bias, (0, pad_len, 0, pad_len), value=0.0)\n",
    "                else:\n",
    "                    postag_bias = postag_bias[:, :, :L, :L]\n",
    "            attn_logits = attn_logits + postag_bias\n",
    "\n",
    "\n",
    "        # Lexicon masking\n",
    "        if lex_mask is not None:\n",
    "            lex_mask = lex_mask.unsqueeze(1).unsqueeze(2)\n",
    "            attn_logits = attn_logits * lex_mask if multiplicative else attn_logits + lex_mask\n",
    "\n",
    "\n",
    "        # Attention masking\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            attn_logits = attn_logits.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = F.softmax(attn_logits, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, V).transpose(1, 2).contiguous().view(B, L, self.dim)\n",
    "        out = self.W_o(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out + residual\n",
    "        if not self.pre_ln:\n",
    "            out = self.norm(out)\n",
    "\n",
    "        return out, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feed-Forward NN with Residual Connection and LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that modern models (T5, DeBERTa, GPT) apply Pre-LN which is LayerNorm before running the first linear combination and also apply 2 dropout after each linear combination to help regularize the parameters to avoid overfitting, useful in large deep models\n",
    "\n",
    "Also modern models utilize the GeLU activation instead of traditional ReLU in the MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GeLU: Gaussian Error Linear Unit has the mathematical definition: \n",
    "\n",
    "$$GeLU(x) = x.\\Theta(x)$$\n",
    "\n",
    "where $\\Theta(x)$ is the cumulative distribution function (CDF) of a standard normal distribution \n",
    "\n",
    "$$GeLU(x) = 0.5x(1 + erf(\\frac{x}{\\sqrt{2}}))$$\n",
    "\n",
    "Instead of making hard decision whether x > 0 in ReLU, GeLU makes a probabilistic decision based on how large x is, this makes the gradient smoother than ReLU and allows smoother transition while partially keeping small negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:32:36.867250Z",
     "iopub.status.busy": "2025-10-28T01:32:36.867092Z",
     "iopub.status.idle": "2025-10-28T01:32:36.889016Z",
     "shell.execute_reply": "2025-10-28T01:32:36.888425Z",
     "shell.execute_reply.started": "2025-10-28T01:32:36.867238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# x = torch.linspace(-5, 5, 200)\n",
    "# relu = F.relu(x)\n",
    "# gelu = F.gelu(x)\n",
    "\n",
    "# plt.plot(x, relu, label='ReLU')\n",
    "# plt.plot(x, gelu, label='GELU')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.title(\"ReLU vs GELU\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:42:54.096237Z",
     "iopub.status.busy": "2025-10-29T14:42:54.095973Z",
     "iopub.status.idle": "2025-10-29T14:42:54.101625Z",
     "shell.execute_reply": "2025-10-29T14:42:54.101009Z",
     "shell.execute_reply.started": "2025-10-29T14:42:54.096217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or 4 * dim  # default expansion that hidden dim is 4 x model_dim\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-LN + MLP + Residual\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return residual + x #Skip connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:44:25.964475Z",
     "iopub.status.busy": "2025-10-29T14:44:25.963729Z",
     "iopub.status.idle": "2025-10-29T14:44:25.972116Z",
     "shell.execute_reply": "2025-10-29T14:44:25.971312Z",
     "shell.execute_reply.started": "2025-10-29T14:44:25.964451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_heads,\n",
    "        dropout=0.1,\n",
    "        ff_hidden_dim=None,\n",
    "        max_distance=256,\n",
    "        pre_ln=True,\n",
    "        num_postags=None,\n",
    "        use_rel_pos=True,    \n",
    "        max_seq_len=5000         \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_rel_pos = use_rel_pos\n",
    "\n",
    "        # Attention + Feedforward\n",
    "        self.attn = MultiHeadAttention(\n",
    "            dim,\n",
    "            n_heads,\n",
    "            dropout=dropout,\n",
    "            pre_ln=pre_ln,\n",
    "            use_rel_pos=use_rel_pos   # pass toggle\n",
    "        )\n",
    "        self.ff = FeedForward(dim, hidden_dim=ff_hidden_dim, dropout=dropout)\n",
    "\n",
    "        # Positional representations\n",
    "        if use_rel_pos:\n",
    "            self.pos_module = RelativePositionBias(max_distance=max_distance, n_heads=n_heads)\n",
    "        else:\n",
    "            self.pos_module = SinusoidalPositionalEncoding(dim, max_len=max_seq_len)\n",
    "\n",
    "        # Optional POS-tag bias\n",
    "        self.postag_bias = POSTag(n_postags=num_postags, n_heads=n_heads) if num_postags is not None else None\n",
    "\n",
    "    def forward(self, x, postag_ids=None, lex_mask=None):\n",
    "        seq_len = x.size(1)\n",
    "        device = x.device\n",
    "\n",
    "        # Get positional representation depending on mode\n",
    "\n",
    "        if self.use_rel_pos:\n",
    "            pos_rep = self.pos_module(seq_len, device=device)  # (n_heads, L, L)\n",
    "            pos_kwargs = {\"pos_bias\": pos_rep, \"sinusoidal_pe\": None}\n",
    "        else:\n",
    "            pos_rep = self.pos_module(seq_len, device=device)  # (1, L, dim)\n",
    "            pos_kwargs = {\"pos_bias\": None, \"sinusoidal_pe\": pos_rep}\n",
    "\n",
    "\n",
    "        # Compute POS-tag bias if applicable\n",
    "        postag_bias = None\n",
    "        if self.postag_bias is not None and postag_ids is not None:\n",
    "            postag_bias = self.postag_bias(postag_ids)  # (B, n_heads, L, L)\n",
    "            postag_bias = postag_bias[..., :seq_len, :seq_len]\n",
    "\n",
    "        #  Self-Attention\n",
    "        x, _ = self.attn(\n",
    "            query=x,\n",
    "            context=None,\n",
    "            postag_bias=postag_bias,\n",
    "            lex_mask=lex_mask,\n",
    "            **pos_kwargs   # dynamically pass correct positional arg\n",
    "        )\n",
    "\n",
    "        #  Feed Forward\n",
    "\n",
    "        x = self.ff(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:46:50.592976Z",
     "iopub.status.busy": "2025-10-29T14:46:50.592494Z",
     "iopub.status.idle": "2025-10-29T14:46:50.598820Z",
     "shell.execute_reply": "2025-10-29T14:46:50.598100Z",
     "shell.execute_reply.started": "2025-10-29T14:46:50.592954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class StackedEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim=768,\n",
    "                 n_heads=12,\n",
    "                 ff_hidden_dim=2048,\n",
    "                 dropout=0.1,\n",
    "                 max_distance=128,\n",
    "                 pre_ln=True,\n",
    "                 num_postags=None,\n",
    "                 use_rel_pos = True,\n",
    "                 num_layers=6):  # number of encoder blocks\n",
    "        super().__init__()\n",
    "\n",
    "        # Stack of independent TransformerEncoder blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoder(\n",
    "                dim=dim,\n",
    "                n_heads=n_heads,\n",
    "                dropout=dropout,\n",
    "                ff_hidden_dim=ff_hidden_dim,\n",
    "                max_distance=max_distance,\n",
    "                pre_ln=pre_ln,\n",
    "                num_postags=num_postags,\n",
    "                use_rel_pos = use_rel_pos\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final normalization — important if using pre-LN blocks (like your TransformerEncoder)\n",
    "        self.final_norm = nn.LayerNorm(dim) if pre_ln else nn.Identity()\n",
    "\n",
    "    def forward(self, x, postag_ids=None, lex_mask=None, output_hidden_states=False):\n",
    "\n",
    "        hidden_states = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, postag_ids=postag_ids, lex_mask=lex_mask)\n",
    "            if output_hidden_states:\n",
    "                hidden_states.append(x)\n",
    "\n",
    "        # Apply normalization after all layers (for pre-LN Transformer)\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            return x, hidden_states\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:46:58.296456Z",
     "iopub.status.busy": "2025-10-29T14:46:58.295893Z",
     "iopub.status.idle": "2025-10-29T14:46:58.303805Z",
     "shell.execute_reply": "2025-10-29T14:46:58.303031Z",
     "shell.execute_reply.started": "2025-10-29T14:46:58.296433Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CombinedEmbedding(nn.Module):\n",
    "    def __init__(self, phobert=None, transformer_encoder=None, alpha=0.5, use_phobert=False, max_len=256):\n",
    "        super().__init__()\n",
    "        self.use_phobert = use_phobert\n",
    "        self.phobert = phobert if use_phobert else None\n",
    "        self.encoder = transformer_encoder\n",
    "        self.alpha = nn.Parameter(torch.tensor(alpha))  # learnable interpolation factor\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, texts, input_ids=None):\n",
    "        batch_size = len(texts)\n",
    "        device = next(self.encoder.parameters()).device\n",
    "        seq_len = self.max_len\n",
    "        hidden_dim = self.encoder.input_dim if hasattr(self.encoder, \"input_dim\") else 768\n",
    "\n",
    "        if self.use_phobert and self.phobert is not None:\n",
    "            pho_hidden, attn_mask, toks = self.phobert.encode(texts)\n",
    "            # truncate/pad pho_hidden if needed\n",
    "            if pho_hidden.size(1) > seq_len:\n",
    "                pho_hidden = pho_hidden[:, :seq_len, :]\n",
    "                attn_mask = attn_mask[:, :seq_len]\n",
    "            elif pho_hidden.size(1) < seq_len:\n",
    "                pad_len = seq_len - pho_hidden.size(1)\n",
    "                pho_hidden = torch.cat([pho_hidden, torch.zeros(batch_size, pad_len, hidden_dim, device=device)], dim=1)\n",
    "                attn_mask = torch.cat([attn_mask, torch.zeros(batch_size, pad_len, device=device)], dim=1)\n",
    "        else:\n",
    "            pho_hidden = torch.zeros(batch_size, seq_len, hidden_dim, device=device)\n",
    "            attn_mask = torch.ones(batch_size, seq_len, device=device)\n",
    "            toks = None\n",
    "\n",
    "        trans_hidden = self.encoder(pho_hidden, attn_mask)\n",
    "        alpha = torch.clamp(self.alpha, 0.0, 1.0)\n",
    "\n",
    "        if not self.use_phobert:\n",
    "            return trans_hidden, attn_mask, toks\n",
    "\n",
    "        combined = alpha * pho_hidden + (1 - alpha) * trans_hidden\n",
    "        return combined, attn_mask, toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:47:44.200364Z",
     "iopub.status.busy": "2025-10-29T14:47:44.199475Z",
     "iopub.status.idle": "2025-10-29T14:47:45.740061Z",
     "shell.execute_reply": "2025-10-29T14:47:45.739246Z",
     "shell.execute_reply.started": "2025-10-29T14:47:44.200339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "if use_phobert == True:\n",
    "    phobert = PhoBertEmbedding(freeze=True)\n",
    "\n",
    "if use_phobert == False:\n",
    "    phobert = None\n",
    "    \n",
    "encoder = StackedEncoder(dim=768, n_heads=12, ff_hidden_dim=2048, dropout=0.1, num_layers=6, max_distance = 128, use_rel_pos = False)\n",
    "\n",
    "combined_model = CombinedEmbedding(None, encoder, alpha=0.5)\n",
    "\n",
    "texts = [\n",
    "    \"Doanh nghiệp có thu nhập chịu thuế quy định tại Điều 3 của Luật này phải nộp thuế.\",\n",
    "    \"Quyết định Căn cứ Nghị định số 55/2025/NĐ-CP ngày 02 tháng 3 năm 2025 của Chính phủ quy định chức năng, nhiệm vụ, quyền hạn và cơ cấu tổ chức của Bộ Khoa học và Công nghệ\"\n",
    "]\n",
    "\n",
    "combined_output, attn_mask, toks = combined_model(texts)\n",
    "print(combined_output.shape)  # (batch, seq_len, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional BiLSTM to catch the semantic structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:47:48.690538Z",
     "iopub.status.busy": "2025-10-29T14:47:48.690081Z",
     "iopub.status.idle": "2025-10-29T14:47:48.695093Z",
     "shell.execute_reply": "2025-10-29T14:47:48.694364Z",
     "shell.execute_reply.started": "2025-10-29T14:47:48.690512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bilstm = nn.LSTM(input_dim, hidden_dim // 2,\n",
    "                              num_layers=num_layers,\n",
    "                              dropout=dropout,\n",
    "                              bidirectional=True,\n",
    "                              batch_first=True)\n",
    "    def forward(self, x):\n",
    "        output, _ = self.bilstm(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Head (for Triplet prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:47:49.450474Z",
     "iopub.status.busy": "2025-10-29T14:47:49.450217Z",
     "iopub.status.idle": "2025-10-29T14:47:49.456213Z",
     "shell.execute_reply": "2025-10-29T14:47:49.455474Z",
     "shell.execute_reply.started": "2025-10-29T14:47:49.450457Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Indenpendent Prediction\n",
    "\n",
    "class DecoderHeads(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_relations):\n",
    "        super().__init__()\n",
    "        self.self_root = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "        self.relation = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, num_relations)\n",
    "        )\n",
    "\n",
    "        # span for start and end indices\n",
    "        self.start = nn.Linear(hidden_dim, 1)\n",
    "        self.end = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pool once for classification\n",
    "        pooled = x.mean(dim=1)  #(batch_size, hidden_dim)\n",
    "\n",
    "        self_root_logits = self.self_root(pooled).squeeze(-1)\n",
    "        relation_logits = self.relation(pooled)\n",
    "\n",
    "        # start and end idx\n",
    "        start_logits = self.start(x).squeeze(-1)\n",
    "        end_logits = self.end(x).squeeze(-1)\n",
    "\n",
    "        return self_root_logits, relation_logits, start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:47:51.779620Z",
     "iopub.status.busy": "2025-10-29T14:47:51.779144Z",
     "iopub.status.idle": "2025-10-29T14:47:51.788125Z",
     "shell.execute_reply": "2025-10-29T14:47:51.787397Z",
     "shell.execute_reply.started": "2025-10-29T14:47:51.779601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Recursive prediction\n",
    "\n",
    "class DecoderHeadCopy(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_relations):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.root = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "        self.relation = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_relations)\n",
    "        )\n",
    "\n",
    "        self.start_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 + num_relations, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.end_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 3 + num_relations, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (B, L, H)\n",
    "        \"\"\"\n",
    "        if x.dim() == 4:\n",
    "            x = x.squeeze(1)  # handle (B, 1, L, H)\n",
    "\n",
    "        # root\n",
    "        root_logits = self.root(x).squeeze(-1)  # (B, L)\n",
    "        root_probs = torch.sigmoid(root_logits)\n",
    "        root_vec = (x * root_probs.unsqueeze(-1)).sum(dim=1) / root_probs.sum(dim=1, keepdim=True).clamp(min=1.0)  # (B, H)\n",
    "\n",
    "        # relation based on root\n",
    "        root_context = root_vec.unsqueeze(1).expand(-1, x.size(1), -1)  # (B, L, H)\n",
    "        rel_input = torch.cat([x, root_context], dim=-1)  # (B, L, 2H)\n",
    "        rel_logits = self.relation(rel_input).mean(dim=1)  # (B, num_rel)\n",
    "        rel_probs = F.softmax(rel_logits, dim=-1)\n",
    "\n",
    "        # start idx based on root and relation\n",
    "        rel_context = rel_probs.unsqueeze(1).expand(-1, x.size(1), -1)  # (B, L, num_rel)\n",
    "        start_input = torch.cat([x, root_context, rel_context], dim=-1)\n",
    "        start_logits = self.start_head(start_input).squeeze(-1)  # (B, L)\n",
    "        start_probs = F.softmax(start_logits, dim=-1)\n",
    "        start_vec = (x * start_probs.unsqueeze(-1)).sum(dim=1)  # (B, H)\n",
    "\n",
    "        # end idx based on root, relation and start idx\n",
    "        start_context = start_vec.unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "        end_input = torch.cat([x, root_context, rel_context, start_context], dim=-1)\n",
    "        end_logits = self.end_head(end_input).squeeze(-1)  # (B, L)\n",
    "\n",
    "        return root_logits, rel_logits, start_logits, end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:47:54.727241Z",
     "iopub.status.busy": "2025-10-29T14:47:54.726710Z",
     "iopub.status.idle": "2025-10-29T14:47:54.733658Z",
     "shell.execute_reply": "2025-10-29T14:47:54.732872Z",
     "shell.execute_reply.started": "2025-10-29T14:47:54.727218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderBody(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim=768,\n",
    "                 num_heads=8,\n",
    "                 ffn_dim=2048,\n",
    "                 dropout=0.1,\n",
    "                 pre_ln=True):\n",
    "        super().__init__()\n",
    "        self.cross_attn = MultiHeadAttention(hidden_dim, num_heads, dropout=dropout, pre_ln=pre_ln)\n",
    "        self.bilstm = BiLSTMEncoder(hidden_dim, hidden_dim)\n",
    "        self.ffn = FeedForward(hidden_dim, ffn_dim)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, decoder_state, encoder_output, mask=None):\n",
    "        # Project decoder_state to match encoder_output dim if needed\n",
    "        if decoder_state.size(-1) != encoder_output.size(-1):\n",
    "            project_to_dim = nn.Linear(decoder_state.size(-1), encoder_output.size([-1])).to(decoder_state.device)\n",
    "            decoder_state = project_to_dim(decoder_state)\n",
    "\n",
    "        # Ensure shape [B, L, D]\n",
    "        if decoder_state.dim() == 2:\n",
    "            decoder_state = decoder_state.unsqueeze(-1).repeat(1, 1, encoder_output.size(-1))\n",
    "\n",
    "        # ---- Cross-Attention ----\n",
    "        cross_out, _ = self.cross_attn(\n",
    "            query=decoder_state,\n",
    "            context=encoder_output,\n",
    "            mask=mask\n",
    "        )\n",
    "        x = self.norm(cross_out + decoder_state)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # ---- BiLSTM + FFN ----\n",
    "        x = self.bilstm(x)\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        return x  # [B, L, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:47:57.987428Z",
     "iopub.status.busy": "2025-10-29T14:47:57.987150Z",
     "iopub.status.idle": "2025-10-29T14:47:57.994270Z",
     "shell.execute_reply": "2025-10-29T14:47:57.993572Z",
     "shell.execute_reply.started": "2025-10-29T14:47:57.987408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, num_heads=8, num_relations=10, ffn_dim=2048, dropout = 0.1, pre_ln = True):\n",
    "        super().__init__()\n",
    "        self.cross_attn = MultiHeadAttention(hidden_dim, num_heads, dropout = dropout, pre_ln = pre_ln)\n",
    "        self.bilstm = BiLSTMEncoder(hidden_dim, hidden_dim)\n",
    "        self.ffn = FeedForward(hidden_dim, ffn_dim)\n",
    "        self.heads_non_rec = DecoderHeads(hidden_dim, num_relations)\n",
    "        self.heads_rec = DecoderHeadCopy(hidden_dim, num_relations)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, decoder_state, encoder_output, mask=None, recursive = None):\n",
    "        \n",
    "        project_to_dim = nn.Linear(decoder_state.size(-1), encoder_output.size(-1))\n",
    "        \n",
    "        # Ensure decoder_state has shape [B, L, D]\n",
    "        if decoder_state.dim() == 2:\n",
    "            # [B, L] → [B, L, D]\n",
    "            decoder_state = decoder_state.unsqueeze(-1).repeat(1, 1, encoder_output.size(-1))\n",
    "        elif decoder_state.dim() == 3 and decoder_state.size(-1) != encoder_output.size(-1):\n",
    "            # adjust dimension mismatch\n",
    "            decoder_state = project_to_dim(decoder_state)\n",
    "\n",
    "        # cross attention\n",
    "        cross_out, _ = self.cross_attn(\n",
    "            query=decoder_state,\n",
    "            context=encoder_output,\n",
    "            mask=mask\n",
    "        )\n",
    "        x = self.norm(cross_out + decoder_state)\n",
    "\n",
    "        x = self.bilstm(x)\n",
    "\n",
    "        # ffn with res-connection\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        # output prediction\n",
    "        if recursive is None:\n",
    "            return self.heads_non_rec(x)\n",
    "        \n",
    "        return self.heads_rec(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:47:58.153173Z",
     "iopub.status.busy": "2025-10-29T14:47:58.152944Z",
     "iopub.status.idle": "2025-10-29T14:47:58.159296Z",
     "shell.execute_reply": "2025-10-29T14:47:58.158580Z",
     "shell.execute_reply.started": "2025-10-29T14:47:58.153157Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class StackedDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim=768,\n",
    "                 num_heads=8,\n",
    "                 num_relations=10,\n",
    "                 ffn_dim=2048,\n",
    "                 dropout=0.1,\n",
    "                 pre_ln=True,\n",
    "                 num_layers=6,\n",
    "                 recursive=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Stack of N DecoderBody blocks (no heads inside)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBody(\n",
    "                hidden_dim=hidden_dim,\n",
    "                num_heads=num_heads,\n",
    "                ffn_dim=ffn_dim,\n",
    "                dropout=dropout,\n",
    "                pre_ln=pre_ln\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final normalization after stacked layers\n",
    "        self.final_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # Shared decoder head\n",
    "        self.head1 = DecoderHeads(hidden_dim, num_relations)\n",
    "        self.head2 = DecoderHeadCopy(hidden_dim, num_relations)\n",
    "        self.recursive = recursive\n",
    "\n",
    "    def forward(self, decoder_state, encoder_output, mask=None):\n",
    "        \"\"\"\n",
    "        decoder_state: [B, L, D] or [B, L]\n",
    "        encoder_output: [B, L, D]\n",
    "        mask: [B, L] optional\n",
    "        \"\"\"\n",
    "\n",
    "        x = decoder_state\n",
    "\n",
    "        # Sequentially pass through each DecoderBody block\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, mask)\n",
    "\n",
    "        # Final normalization\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        # Apply the shared decoder head\n",
    "        if self.recursive:\n",
    "            return self.head2(x)\n",
    "            \n",
    "        return self.head1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:00.768391Z",
     "iopub.status.busy": "2025-10-29T14:48:00.767671Z",
     "iopub.status.idle": "2025-10-29T14:48:00.774597Z",
     "shell.execute_reply": "2025-10-29T14:48:00.773659Z",
     "shell.execute_reply.started": "2025-10-29T14:48:00.768366Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 phobert_embedder=None,\n",
    "                 encoder=None,\n",
    "                 decoder=None,\n",
    "                 alpha=0.5,\n",
    "                 use_phobert=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_phobert = use_phobert\n",
    "        self.embedding = phobert_embedder  \n",
    "        self.encoder = encoder              # StackedEncoder\n",
    "        self.decoder = decoder              # StackedDecoder\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, texts, postag_ids):\n",
    "\n",
    "        combined_output, attn_mask, toks = self.embedding(texts)\n",
    "        # combined_output: (B, L, D)\n",
    "        # attn_mask:      (B, L)\n",
    "\n",
    "        #multi-block encoder\n",
    "        encoded_output = self.encoder(\n",
    "            combined_output,\n",
    "            postag_ids=postag_ids,\n",
    "            lex_mask=attn_mask\n",
    "        )\n",
    "\n",
    "        decoder_output = self.decoder(\n",
    "            decoder_state=encoded_output,\n",
    "            encoder_output=encoded_output,\n",
    "            mask=attn_mask\n",
    "        )\n",
    "\n",
    "        # if StackedDecoder returns logits directly (root, rel, start, end), unpack them here:\n",
    "        if isinstance(decoder_output, tuple) and len(decoder_output) == 4:\n",
    "            root_logits, relation_logits, start_logits, end_logits = decoder_output\n",
    "        else:\n",
    "            # if stacked decoder only returns the last hidden state\n",
    "            root_logits = relation_logits = start_logits = end_logits = None\n",
    "\n",
    "        return {\n",
    "            \"self_root\": root_logits,\n",
    "            \"relation\": relation_logits,\n",
    "            \"start_logits\": start_logits,\n",
    "            \"end_logits\": end_logits,\n",
    "            \"mask\": attn_mask,\n",
    "            \"tokens\": toks\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:05.020211Z",
     "iopub.status.busy": "2025-10-29T14:48:05.019746Z",
     "iopub.status.idle": "2025-10-29T14:48:05.392732Z",
     "shell.execute_reply": "2025-10-29T14:48:05.391901Z",
     "shell.execute_reply.started": "2025-10-29T14:48:05.020189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('D:/Study/Education/Projects/Group_Project/rag_model/model/RE/RE_training_final.csv')\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/traindataset/RE_training_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:09.365679Z",
     "iopub.status.busy": "2025-10-28T01:33:09.365423Z",
     "iopub.status.idle": "2025-10-28T01:33:09.369415Z",
     "shell.execute_reply": "2025-10-28T01:33:09.368705Z",
     "shell.execute_reply.started": "2025-10-28T01:33:09.365657Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# #Process Postagging\n",
    "\n",
    "# import ast\n",
    "# df['postag'] = df['postag'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# unique_tags = sorted({tag for tags in df['postag'] for tag in tags})\n",
    "\n",
    "# pos2idx = {tag: idx for idx, tag in enumerate(unique_tags, start=1)}  # start=1 to reserve 0 for padding\n",
    "# idx2pos = {idx: tag for tag, idx in pos2idx.items()}\n",
    "\n",
    "# df['postag_idx'] = df['postag'].apply(lambda tags: [pos2idx[tag] for tag in tags])\n",
    "\n",
    "# print(pos2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:08.164518Z",
     "iopub.status.busy": "2025-10-29T14:48:08.163891Z",
     "iopub.status.idle": "2025-10-29T14:48:08.178597Z",
     "shell.execute_reply": "2025-10-29T14:48:08.177890Z",
     "shell.execute_reply.started": "2025-10-29T14:48:08.164498Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Amended': 0, 'Drop': 1, 'Guide': 2, 'Others': 3, 'PartDrop': 4, 'PartReplace': 5, 'Pursuant': 6, 'Reference': 7, 'Replace': 8}\n"
     ]
    }
   ],
   "source": [
    "relations = sorted(df[\"relation\"].unique())\n",
    "relation2id = {rel: idx for idx, rel in enumerate(relations)}\n",
    "id2relation = {v: k for k, v in relation2id.items()}\n",
    "\n",
    "print(relation2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:27.024778Z",
     "iopub.status.busy": "2025-10-29T14:48:27.024499Z",
     "iopub.status.idle": "2025-10-29T14:48:28.947620Z",
     "shell.execute_reply": "2025-10-29T14:48:28.946798Z",
     "shell.execute_reply.started": "2025-10-29T14:48:27.024759Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhoBERT enabled. Using PhoBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 256])\n",
      "torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "### Test with Postag\n",
    "#Require definitions of df first with the existence of postag_ids\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = 'cuda' #when run on local\n",
    "\n",
    "texts = df['input_text'].iloc[:2].tolist()\n",
    "# postags = df['postag'].iloc[:2].tolist()\n",
    "# pos_ids = [torch.tensor(p, dtype=torch.long) for p in df['postag_idx'].iloc[:2]]\n",
    "\n",
    "# pos_ids = pad_sequence(pos_ids, batch_first=True, padding_value=0).long().to(device)\n",
    "\n",
    "if use_phobert:\n",
    "    phobert = PhoBertEmbedding(freeze=True, max_length=256)\n",
    "    phobert.to(device)\n",
    "    print('PhoBERT enabled. Using PhoBERT')\n",
    "else:\n",
    "    phobert = None\n",
    "    print(\"PhoBERT disabled. Using encoder-only embeddings.\")\n",
    "\n",
    "# initialize encoder & decoder \n",
    "encoder = StackedEncoder(\n",
    "    dim=768,\n",
    "    n_heads=8,\n",
    "    ff_hidden_dim=2048,\n",
    "    dropout=0.1,\n",
    "    # num_postags=len(pos2idx),\n",
    "    num_layers=1,\n",
    "    use_rel_pos = False\n",
    ").to(device)\n",
    "\n",
    "decoder = StackedDecoder(\n",
    "    hidden_dim=768,\n",
    "    num_heads=8,\n",
    "    num_relations=10,\n",
    "    num_layers=3,\n",
    "    recursive=False\n",
    ").to(device)\n",
    "\n",
    "\n",
    "embedding = CombinedEmbedding(\n",
    "    phobert=phobert,\n",
    "    transformer_encoder=encoder,\n",
    "    alpha=0.5,\n",
    "    use_phobert=use_phobert\n",
    ")\n",
    "\n",
    "model = Transformer(\n",
    "    phobert_embedder=embedding,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    alpha=0.5,\n",
    "    use_phobert=use_phobert\n",
    ").to(device)\n",
    "\n",
    "outputs = model(texts, postag_ids=None)\n",
    "\n",
    "print(outputs[\"self_root\"].shape)    # torch.Size([B])\n",
    "print(outputs[\"relation\"].shape)     # torch.Size([B, num_relations])\n",
    "print(outputs[\"start_logits\"].shape) # torch.Size([B, seq_len])\n",
    "print(outputs[\"end_logits\"].shape)   # torch.Size([B, seq_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:31.913843Z",
     "iopub.status.busy": "2025-10-29T14:48:31.913578Z",
     "iopub.status.idle": "2025-10-29T14:48:31.920637Z",
     "shell.execute_reply": "2025-10-29T14:48:31.919903Z",
     "shell.execute_reply.started": "2025-10-29T14:48:31.913824Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 172.7M, Trainable: 37.7M\n"
     ]
    }
   ],
   "source": [
    "total = sum(p.numel() for p in model.parameters())\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total params: {total/1e6:.1f}M, Trainable: {trainable/1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.213818Z",
     "iopub.status.busy": "2025-10-28T01:33:12.213587Z",
     "iopub.status.idle": "2025-10-28T01:33:12.258472Z",
     "shell.execute_reply": "2025-10-28T01:33:12.257932Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.213779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df = gs_to_df_pandas('RE_training')\n",
    "\n",
    "# df['input_text'] = df['input_text'].apply(lambda x: x.replace('_', ' '))\n",
    "\n",
    "# df['second_entity'] = df['second_entity'].apply(lambda x: x.replace('_', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.259162Z",
     "iopub.status.busy": "2025-10-28T01:33:12.258992Z",
     "iopub.status.idle": "2025-10-28T01:33:12.274458Z",
     "shell.execute_reply": "2025-10-28T01:33:12.273969Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.259141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.275284Z",
     "iopub.status.busy": "2025-10-28T01:33:12.275056Z",
     "iopub.status.idle": "2025-10-28T01:33:12.291506Z",
     "shell.execute_reply": "2025-10-28T01:33:12.290824Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.275265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def get_word_indices(row):\n",
    "#     # Clean punctuation and normalize spaces\n",
    "#     text = re.sub(r'[^\\w\\s]', '', row['input_text'])\n",
    "#     entity = re.sub(r'[^\\w\\s]', '', row['second_entity'])\n",
    "    \n",
    "#     text_words = text.split()\n",
    "#     entity_words = entity.split()\n",
    "    \n",
    "#     n = len(entity_words)\n",
    "    \n",
    "#     # Find first matching subsequence\n",
    "#     for i in range(len(text_words) - n + 1):\n",
    "#         if text_words[i:i+n] == entity_words:\n",
    "#             return pd.Series({'start_word_idx': i, 'end_word_idx': i + n - 1})\n",
    "    \n",
    "#     # Not found\n",
    "#     return pd.Series({'start_word_idx': -1, 'end_word_idx': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.292492Z",
     "iopub.status.busy": "2025-10-28T01:33:12.292255Z",
     "iopub.status.idle": "2025-10-28T01:33:12.314703Z",
     "shell.execute_reply": "2025-10-28T01:33:12.314173Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.292472Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df[[\"start_idx\", \"end_idx\"]] = df.apply(get_word_indices, axis=1).astype(int)\n",
    "\n",
    "# df['first_entity'] = df['first_entity'].apply(lambda x: 1 if x == 'self' else 0)\n",
    "\n",
    "# df = df.rename(columns={'first_entity': 'self_root'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.315588Z",
     "iopub.status.busy": "2025-10-28T01:33:12.315413Z",
     "iopub.status.idle": "2025-10-28T01:33:12.330169Z",
     "shell.execute_reply": "2025-10-28T01:33:12.329642Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.315575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.331100Z",
     "iopub.status.busy": "2025-10-28T01:33:12.330768Z",
     "iopub.status.idle": "2025-10-28T01:33:12.346450Z",
     "shell.execute_reply": "2025-10-28T01:33:12.345964Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.331079Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# write_df_to_gs(df, 'RE_training_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add extra input feature from text - postagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.347303Z",
     "iopub.status.busy": "2025-10-28T01:33:12.347068Z",
     "iopub.status.idle": "2025-10-28T01:33:12.362953Z",
     "shell.execute_reply": "2025-10-28T01:33:12.362451Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.347280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# postag = df['input_text'].apply(lambda x: annotator.annotate(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.363670Z",
     "iopub.status.busy": "2025-10-28T01:33:12.363455Z",
     "iopub.status.idle": "2025-10-28T01:33:12.378978Z",
     "shell.execute_reply": "2025-10-28T01:33:12.378265Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.363655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def create_pos(text):   \n",
    "#     pos_tagging = []\n",
    "\n",
    "#     pos = annotator.annotate(text)\n",
    "\n",
    "#     for result_item in pos:\n",
    "\n",
    "#         sentence_segments = result_item['sentences'][0]\n",
    "        \n",
    "#         current_sentence_tags = []\n",
    "#         for segment in sentence_segments:\n",
    "#             word_form = segment['form']\n",
    "#             pos_tag = segment['posTag']\n",
    "            \n",
    "#             sub_words = word_form.split('_')\n",
    "#             repeated_tags = [pos_tag] * len(sub_words)\n",
    "#             current_sentence_tags.extend(repeated_tags)\n",
    "#         pos_tagging.append(current_sentence_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.379900Z",
     "iopub.status.busy": "2025-10-28T01:33:12.379640Z",
     "iopub.status.idle": "2025-10-28T01:33:12.394366Z",
     "shell.execute_reply": "2025-10-28T01:33:12.393657Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.379879Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print('Original text:')\n",
    "# print(df['input_text'][0])\n",
    "# print('Postag:')\n",
    "# print(pos_tagging[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T01:33:12.395348Z",
     "iopub.status.busy": "2025-10-28T01:33:12.395143Z",
     "iopub.status.idle": "2025-10-28T01:33:12.410145Z",
     "shell.execute_reply": "2025-10-28T01:33:12.409423Z",
     "shell.execute_reply.started": "2025-10-28T01:33:12.395334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df['postag'] = pos_tagging\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset and Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:41.137621Z",
     "iopub.status.busy": "2025-10-29T14:48:41.137355Z",
     "iopub.status.idle": "2025-10-29T14:48:41.145588Z",
     "shell.execute_reply": "2025-10-29T14:48:41.144796Z",
     "shell.execute_reply.started": "2025-10-29T14:48:41.137602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class trainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, relation2id, max_length=256):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.relation2id = relation2id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row[\"input_text\"]\n",
    "\n",
    "        # Tokenize text\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        input_ids = encoded[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze(0)\n",
    "        seq_len = int(attention_mask.sum())\n",
    "\n",
    "        # # POS tagging indices\n",
    "        # postag_idx = row[\"postag_idx\"]\n",
    "        # if not isinstance(postag_idx, list):\n",
    "        #     postag_idx = [0]  # fallback if something went wrong\n",
    "\n",
    "        # postag_tensor = torch.tensor(postag_idx, dtype=torch.long)\n",
    "        # pad/truncate POS tag sequence to match tokenizer length\n",
    "        # if len(postag_tensor) < self.max_length:\n",
    "        #     pad_len = self.max_length - len(postag_tensor)\n",
    "        #     postag_tensor = torch.cat([postag_tensor, torch.zeros(pad_len, dtype=torch.long)])\n",
    "        # else:\n",
    "        #     postag_tensor = postag_tensor[:self.max_length]\n",
    "\n",
    "        # Start/End indices sanity check \n",
    "        start_idx = int(row[\"start_idx\"])\n",
    "        end_idx = int(row[\"end_idx\"])\n",
    "        if start_idx < 0 or end_idx < 0 or start_idx >= seq_len or end_idx >= seq_len:\n",
    "            print(f\"[WARN] invalid span indices at row {idx}: ({start_idx}, {end_idx}) | seq_len={seq_len}\")\n",
    "            start_idx, end_idx = 0, 0  # fallback inside range\n",
    "\n",
    "        labels = {\n",
    "            \"self_root\": torch.tensor(row[\"self_root\"], dtype=torch.float),\n",
    "            \"relation\": torch.tensor(self.relation2id[row[\"relation\"]], dtype=torch.long),\n",
    "            \"start\": torch.tensor(start_idx, dtype=torch.long),\n",
    "            \"end\": torch.tensor(end_idx, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            # \"postag_ids\": postag_tensor,\n",
    "            \"text\": text,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:41.375704Z",
     "iopub.status.busy": "2025-10-29T14:48:41.375433Z",
     "iopub.status.idle": "2025-10-29T14:48:41.395289Z",
     "shell.execute_reply": "2025-10-29T14:48:41.394618Z",
     "shell.execute_reply.started": "2025-10-29T14:48:41.375683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid spans: 0\n",
      "✅ Remaining rows after drop: 32380\n"
     ]
    }
   ],
   "source": [
    "#Drop rows with no traced second entity and invalid entity span (outside of max_length)\n",
    "\n",
    "bad_rows = df[(df[\"start_idx\"] < 0) | (df[\"end_idx\"] < 0)]\n",
    "print(\"Invalid spans:\", len(bad_rows))\n",
    "\n",
    "df = df.drop(bad_rows.index).reset_index(drop=True)\n",
    "df = df[(df[\"end_idx\"] < 256)] #current max length\n",
    "print(\"✅ Remaining rows after drop:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:43.151156Z",
     "iopub.status.busy": "2025-10-29T14:48:43.150889Z",
     "iopub.status.idle": "2025-10-29T14:48:43.167677Z",
     "shell.execute_reply": "2025-10-29T14:48:43.166910Z",
     "shell.execute_reply.started": "2025-10-29T14:48:43.151137Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Căn cứ Hiến pháp nước Cộng hòa xã hội chủ nghĩa Việt Nam;\n",
      "{'self_root': tensor(1.), 'relation': tensor(6), 'start': tensor(2), 'end': tensor(12)}\n"
     ]
    }
   ],
   "source": [
    "dataset = trainDataset(df, tokenizer, relation2id)\n",
    "\n",
    "sample = dataset[0]\n",
    "print(sample[\"text\"])\n",
    "print(sample[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:43.463639Z",
     "iopub.status.busy": "2025-10-29T14:48:43.462906Z",
     "iopub.status.idle": "2025-10-29T14:48:43.471324Z",
     "shell.execute_reply": "2025-10-29T14:48:43.470583Z",
     "shell.execute_reply.started": "2025-10-29T14:48:43.463608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "total_size = len(dataset)\n",
    "test_size = int(0.2 * total_size)\n",
    "train_size = total_size - test_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T14:48:45.520090Z",
     "iopub.status.busy": "2025-10-29T14:48:45.519629Z",
     "iopub.status.idle": "2025-10-29T14:48:45.525412Z",
     "shell.execute_reply": "2025-10-29T14:48:45.524207Z",
     "shell.execute_reply.started": "2025-10-29T14:48:45.520046Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches: 3238\n",
      "Number of test batches: 810\n"
     ]
    }
   ],
   "source": [
    "num_batches = len(train_loader)\n",
    "print(f\"Number of train batches: {num_batches}\")\n",
    "\n",
    "num_batches2 = len(test_loader)\n",
    "print(f\"Number of test batches: {num_batches2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-29T15:30:42.918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "criterion_root = torch.nn.BCEWithLogitsLoss()       # for root\n",
    "criterion_relation = torch.nn.CrossEntropyLoss()    # for relation\n",
    "criterion_span = torch.nn.CrossEntropyLoss()        # for start/end positions\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "device = next(model.parameters()).device  # ensures correct device\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        texts = batch[\"text\"]\n",
    "\n",
    "        # Safe conversion for postag_ids\n",
    "        # postag_ids = batch[\"postag_ids\"].to(model.device if hasattr(model, \"device\") else \"cuda\")\n",
    "\n",
    "        outputs = model(texts, postag_ids=None)\n",
    "\n",
    "        # Unpack model outputs\n",
    "        self_root_logits = outputs[\"self_root\"]\n",
    "        relation_logits = outputs[\"relation\"]\n",
    "        start_logits = outputs[\"start_logits\"]\n",
    "        end_logits = outputs[\"end_logits\"]\n",
    "\n",
    "        labels = batch[\"labels\"]\n",
    "        self_root_labels = labels[\"self_root\"].to(self_root_logits.device)\n",
    "        relation_labels = labels[\"relation\"].to(relation_logits.device)\n",
    "        start_labels = labels[\"start\"].to(start_logits.device)\n",
    "        end_labels = labels[\"end\"].to(end_logits.device)\n",
    "\n",
    "        # Compute loss\n",
    "        loss_self_root = criterion_root(self_root_logits.squeeze(), self_root_labels)\n",
    "        loss_relation = criterion_relation(relation_logits, relation_labels)\n",
    "        loss_start = criterion_span(start_logits, start_labels)\n",
    "        loss_end = criterion_span(end_logits, end_labels)\n",
    "\n",
    "        total_batch_loss = loss_self_root + loss_relation + 0.5 * (loss_start + loss_end)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += total_batch_loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), 'state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T05:40:43.253672Z",
     "iopub.status.busy": "2025-10-28T05:40:43.253068Z",
     "iopub.status.idle": "2025-10-28T05:44:27.705905Z",
     "shell.execute_reply": "2025-10-28T05:44:27.705220Z",
     "shell.execute_reply.started": "2025-10-28T05:40:43.253647Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_root:   Accuracy: 0.9969 | Precision: 0.9847 | Recall: 0.9973 | F1: 0.9909\n",
      "relation:    Accuracy: 0.9617 | Macro-F1: 0.8249\n",
      "span         Precision: 0.8379 | Recall: 0.8379 | F1: 0.8379\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "root_preds, root_labels = [], []\n",
    "rel_preds, rel_labels = [], []\n",
    "pred_spans, true_spans = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:   \n",
    "        texts = batch[\"text\"]\n",
    "        outputs = model(texts, postag_ids = None)\n",
    "\n",
    "        # self_root\n",
    "        self_root_logits = outputs[\"self_root\"].squeeze()\n",
    "        self_root_labels = batch[\"labels\"][\"self_root\"].to(self_root_logits.device)\n",
    "        preds_root = (torch.sigmoid(self_root_logits) > 0.5).long()\n",
    "\n",
    "        root_preds.extend(preds_root.cpu().numpy())\n",
    "        root_labels.extend(self_root_labels.cpu().numpy())\n",
    "\n",
    "        # relation\n",
    "        relation_logits = outputs[\"relation\"]\n",
    "        relation_labels = batch[\"labels\"][\"relation\"].to(relation_logits.device)\n",
    "        preds_relation = torch.argmax(relation_logits, dim=1)\n",
    "\n",
    "        rel_preds.extend(preds_relation.cpu().numpy())\n",
    "        rel_labels.extend(relation_labels.cpu().numpy())\n",
    "\n",
    "        # span\n",
    "        start_logits = outputs[\"start_logits\"]\n",
    "        end_logits = outputs[\"end_logits\"]\n",
    "        start_labels = batch[\"labels\"][\"start\"].to(start_logits.device)\n",
    "        end_labels = batch[\"labels\"][\"end\"].to(end_logits.device)\n",
    "\n",
    "        pred_start = torch.argmax(start_logits, dim=1)\n",
    "        pred_end = torch.argmax(end_logits, dim=1)\n",
    "\n",
    "        for s_pred, e_pred, s_true, e_true in zip(pred_start, pred_end, start_labels, end_labels):\n",
    "            pred_spans.append((s_pred.item(), e_pred.item()))\n",
    "            true_spans.append((s_true.item(), e_true.item()))\n",
    "\n",
    "\n",
    "root_precision = precision_score(root_labels, root_preds)\n",
    "root_recall = recall_score(root_labels, root_preds)\n",
    "root_f1 = f1_score(root_labels, root_preds)\n",
    "root_acc = accuracy_score(root_labels, root_preds)\n",
    "\n",
    "rel_acc = accuracy_score(rel_labels, rel_preds)\n",
    "rel_f1 = f1_score(rel_labels, rel_preds, average='macro')\n",
    "\n",
    "correct_spans = sum([1 for p, t in zip(pred_spans, true_spans) if p == t])\n",
    "span_precision = correct_spans / len(pred_spans)\n",
    "span_recall = correct_spans / len(true_spans)\n",
    "span_f1 = 2 * span_precision * span_recall / (span_precision + span_recall + 1e-8)\n",
    "\n",
    "\n",
    "print(f\"self_root:   Accuracy: {root_acc:.4f} | Precision: {root_precision:.4f} | Recall: {root_recall:.4f} | F1: {root_f1:.4f}\")\n",
    "print(f\"relation:    Accuracy: {rel_acc:.4f} | Macro-F1: {rel_f1:.4f}\")\n",
    "print(f\"span         Precision: {span_precision:.4f} | Recall: {span_recall:.4f} | F1: {span_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8544927,
     "sourceId": 13461753,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8583049,
     "sourceId": 13518026,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 478689,
     "modelInstanceId": 462903,
     "sourceId": 615892,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 482913,
     "modelInstanceId": 467089,
     "sourceId": 621071,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
